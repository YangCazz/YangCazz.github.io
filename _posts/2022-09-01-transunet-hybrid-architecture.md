---
layout: post
title: "TransUNetï¼šå½“Transformeré‡è§åŒ»å­¦å›¾åƒåˆ†å‰²"
date: 2022-09-01 10:00:00 +0800
categories: [åŒ»å­¦å½±åƒ, å›¾åƒåˆ†å‰²]
tags: [UNet, Transformer, åŒ»å­¦å›¾åƒ]
excerpt: "æ¢ç´¢TransUNetå¦‚ä½•å°†Transformerçš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ä¸CNNçš„å±€éƒ¨ç‰¹å¾æå–ç›¸ç»“åˆï¼Œå¼€å¯åŒ»å­¦å›¾åƒåˆ†å‰²çš„Transformeræ—¶ä»£ã€‚"
author: YangCazz
math: true
---

## ğŸ“‹ å¼•è¨€

åœ¨å‰é¢çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†åŸºäºCNNçš„å„ç§UNetå˜ç§ï¼š[æ ‡å‡†UNet](/2025/02/01/fcn-unet-foundation/)ã€[Attention UNet](/2025/02/10/attention-unet/)ã€[UNet++/UNet 3+](/2025/02/15/unet-plus-series/)ã€‚è¿™äº›æ–¹æ³•è™½ç„¶å¼ºå¤§ï¼Œä½†éƒ½å­˜åœ¨ä¸€ä¸ªæ ¹æœ¬æ€§é™åˆ¶ï¼š

**CNNçš„å±€éƒ¨æ„Ÿå—é‡**

```
é—®é¢˜ï¼šå·ç§¯æ“ä½œåªèƒ½çœ‹åˆ°å±€éƒ¨åŒºåŸŸ

3Ã—3å·ç§¯ï¼šæ„Ÿå—é‡3Ã—3
å †å 5å±‚ï¼šæ„Ÿå—é‡ä»…11Ã—11
                â†“
          éš¾ä»¥å»ºæ¨¡è¿œè·ç¦»ä¾èµ–
```

**åŒ»å­¦å›¾åƒçš„æŒ‘æˆ˜**ï¼š
- å™¨å®˜ç»“æ„è·¨è¶Šå¤§èŒƒå›´ï¼ˆå¦‚ä¸»åŠ¨è„‰ä»å¿ƒè„å»¶ä¼¸åˆ°è…¹éƒ¨ï¼‰
- ç—…ç¶ä½ç½®éœ€è¦å…¨å±€ä¸Šä¸‹æ–‡ï¼ˆå¦‚è½¬ç§»ç˜¤çš„ç›¸å¯¹ä½ç½®ï¼‰
- å¤šå™¨å®˜åˆ†å‰²éœ€è¦ç†è§£ç©ºé—´å…³ç³»

**TransUNet**ï¼ˆ2021ï¼‰å¼•å…¥**Transformer**ï¼Œå®ç°çœŸæ­£çš„**å…¨å±€å»ºæ¨¡**ï¼š
- âœ… è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼šä»»æ„ä¸¤ç‚¹ç›´æ¥äº¤äº’
- âœ… é•¿è·ç¦»ä¾èµ–ï¼šæ— éœ€å †å å¤šå±‚å³å¯æ„ŸçŸ¥å…¨å›¾
- âœ… CNN + Transformeræ··åˆï¼šå…¼é¡¾å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€è¯­ä¹‰

---

## ğŸ”¬ TransformeråŸºç¡€å›é¡¾

åœ¨æ·±å…¥TransUNetä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿå›é¡¾Transformerçš„æ ¸å¿ƒæœºåˆ¶ã€‚

### è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šè®¡ç®—æ¯ä¸ªä½ç½®ä¸æ‰€æœ‰ä½ç½®çš„ç›¸å…³æ€§ã€‚

ç»™å®šè¾“å…¥åºåˆ— \( X \in \mathbb{R}^{N \times D} \)ï¼ˆ\(N\)ä¸ªtokenï¼Œæ¯ä¸ªç»´åº¦\(D\)ï¼‰ï¼š

**æ­¥éª¤1ï¼šçº¿æ€§å˜æ¢**

$$
\begin{aligned}
Q &= XW_Q \in \mathbb{R}^{N \times d_k} \quad \text{(Query)} \\
K &= XW_K \in \mathbb{R}^{N \times d_k} \quad \text{(Key)} \\
V &= XW_V \in \mathbb{R}^{N \times d_v} \quad \text{(Value)}
\end{aligned}
$$

**æ­¥éª¤2ï¼šè®¡ç®—æ³¨æ„åŠ›æƒé‡**

$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$

- \( QK^T \in \mathbb{R}^{N \times N} \)ï¼šç›¸ä¼¼åº¦çŸ©é˜µ
- \( \text{softmax} \)ï¼šå½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
- ä¹˜ä»¥ \( V \)ï¼šåŠ æƒèšåˆç‰¹å¾

**å¯è§†åŒ–**ï¼š

```
è¾“å…¥ï¼š[x1, x2, x3, x4]

æ³¨æ„åŠ›çŸ©é˜µï¼ˆQK^Tï¼‰ï¼š
     x1   x2   x3   x4
x1 [0.8  0.1  0.05 0.05]  â† x1ä¸»è¦å…³æ³¨è‡ªå·±
x2 [0.1  0.6  0.2  0.1 ]  â† x2å…³æ³¨x2å’Œx3
x3 [0.05 0.3  0.5  0.15]  â† x3å…³æ³¨x2å’Œx3
x4 [0.05 0.1  0.1  0.75]  â† x4ä¸»è¦å…³æ³¨è‡ªå·±

è¾“å‡ºï¼šæ¯ä¸ªä½ç½®æ˜¯æ‰€æœ‰ä½ç½®çš„åŠ æƒå’Œ
```

**å…³é”®ä¼˜åŠ¿**ï¼š
- âœ… æ¯ä¸ªtokenå¯ä»¥ç›´æ¥çœ‹åˆ°æ‰€æœ‰tokenï¼ˆå…¨å±€æ„Ÿå—é‡ï¼‰
- âœ… è®¡ç®—å¹¶è¡ŒåŒ–ï¼ˆä¸åƒRNNéœ€è¦ä¸²è¡Œï¼‰
- âœ… æƒé‡å¯è§†åŒ–ï¼ˆå¯è§£é‡Šæ€§ï¼‰

### Multi-Head Attention

**æ€æƒ³**ï¼šå¤šä¸ªæ³¨æ„åŠ›å¤´å­¦ä¹ ä¸åŒçš„äº¤äº’æ¨¡å¼ã€‚

$$
\begin{aligned}
\text{head}_i &= \text{Attention}(XW_Q^i, XW_K^i, XW_V^i) \\
\text{MultiHead}(X) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W_O
\end{aligned}
$$

**ç¤ºä¾‹**ï¼š8ä¸ªhead
- Head 1ï¼šå…³æ³¨å±€éƒ¨é‚»åŸŸ
- Head 2ï¼šå…³æ³¨é•¿è·ç¦»ä¾èµ–
- Head 3ï¼šå…³æ³¨å¯¹ç§°ä½ç½®
- ...

### Transformer Block

```
Input
  â†“
Multi-Head Self-Attention
  â†“
Add & Norm (æ®‹å·®è¿æ¥)
  â†“
Feed-Forward Network (MLP)
  â†“
Add & Norm
  â†“
Output
```

**å®Œæ•´å…¬å¼**ï¼š

$$
\begin{aligned}
Z' &= \text{LayerNorm}(X + \text{MultiHeadAttention}(X)) \\
Z &= \text{LayerNorm}(Z' + \text{FFN}(Z'))
\end{aligned}
$$

å…¶ä¸­ FFNï¼ˆFeed-Forward Networkï¼‰ï¼š

$$
\text{FFN}(Z) = \text{GELU}(ZW_1 + b_1)W_2 + b_2
$$

---

## ğŸ¯ TransUNetï¼šæ··åˆæ¶æ„

### è®ºæ–‡ä¿¡æ¯
- **æ ‡é¢˜**: TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
- **ä½œè€…**: Jieneng Chen, et al. (Johns Hopkins University)
- **å‘è¡¨**: arXiv 2021
- **è®ºæ–‡é“¾æ¥**: [arXiv:2102.04306](https://arxiv.org/abs/2102.04306)
- **å®˜æ–¹ä»£ç **: [GitHub](https://github.com/Beckschen/TransUNet)

### æ•´ä½“æ¶æ„

TransUNet = **CNNç¼–ç å™¨** + **Transformer** + **CNNè§£ç å™¨**

```
è¾“å…¥å›¾åƒ (HÃ—WÃ—3)
      â†“
====================
CNNç¼–ç å™¨ï¼ˆé™é‡‡æ ·ï¼‰
====================
Conv 3Ã—3  â†’  (H/2Ã—W/2Ã—64)   [e1]
      â†“
Pool + Conv  â†’  (H/4Ã—W/4Ã—128)  [e2]
      â†“
Pool + Conv  â†’  (H/8Ã—W/8Ã—256)  [e3]
      â†“
====================
Transformerå±‚
====================
Patch Embedding  â†’  (PÃ—P tokens, Dç»´)
      â†“
Transformer Ã— L  â†’  (PÃ—P tokens, Dç»´)
      â†“
Reshape  â†’  (H/8Ã—W/8Ã—D)  [bottleneck]
      â†“
====================
CNNè§£ç å™¨ï¼ˆä¸Šé‡‡æ ·ï¼‰
====================
UpConv + e3  â†’  (H/4Ã—W/4Ã—256)
      â†“
UpConv + e2  â†’  (H/2Ã—W/2Ã—128)
      â†“
UpConv + e1  â†’  (HÃ—WÃ—64)
      â†“
Conv 1Ã—1  â†’  (HÃ—WÃ—num_classes)
```

### å…³é”®ç»„ä»¶è¯¦è§£

#### 1. Patch Embedding

**é—®é¢˜**ï¼šTransformerå¤„ç†åºåˆ—ï¼Œå¦‚ä½•å°†2Då›¾åƒè½¬æ¢ä¸ºåºåˆ—ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼šå°†å›¾åƒåˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„patchï¼Œå±•å¹³ä¸ºtokenåºåˆ—ã€‚

```python
class PatchEmbedding(nn.Module):
    def __init__(self, in_channels=256, embed_dim=768, patch_size=1):
        super().__init__()
        self.proj = nn.Conv2d(
            in_channels, 
            embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )
    
    def forward(self, x):
        # x: (B, 256, H/8, W/8)
        x = self.proj(x)  # (B, 768, H/8, W/8)
        
        # å±•å¹³ä¸ºåºåˆ—
        B, C, H, W = x.shape
        x = x.flatten(2)  # (B, 768, H*W)
        x = x.transpose(1, 2)  # (B, H*W, 768)
        
        return x, (H, W)  # è¿”å›åºåˆ—å’Œç©ºé—´å°ºå¯¸
```

**æ•°å­¦è¡¨ç¤º**ï¼š

è®¾è¾“å…¥ç‰¹å¾å›¾ \( F \in \mathbb{R}^{H \times W \times C} \)ï¼Œpatchå¤§å°ä¸º \( p \times p \)ã€‚

$$
\begin{aligned}
\text{num\_patches} &= \frac{H}{p} \times \frac{W}{p} \\
\text{patch}_i &= \text{Flatten}(F[i \cdot p : (i+1) \cdot p, :]) \in \mathbb{R}^{p^2 C} \\
\text{embedding}_i &= \text{patch}_i \cdot W_{\text{proj}} \in \mathbb{R}^D
\end{aligned}
$$

**TransUNetçš„é€‰æ‹©**ï¼špatch_size = 1ï¼ˆæ¯ä¸ªåƒç´ ä¸€ä¸ªtokenï¼‰

#### 2. Positional Encoding

**é—®é¢˜**ï¼šTransformeræ˜¯æ’åˆ—ä¸å˜çš„ï¼ˆpermutation-invariantï¼‰ï¼Œæ— æ³•åŒºåˆ†ä½ç½®ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šæ·»åŠ ä½ç½®ç¼–ç ã€‚

```python
class PositionalEncoding2D(nn.Module):
    def __init__(self, num_patches, embed_dim):
        super().__init__()
        # å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥
        self.pos_embed = nn.Parameter(
            torch.zeros(1, num_patches, embed_dim)
        )
    
    def forward(self, x):
        # x: (B, N, D)
        return x + self.pos_embed
```

**ä¸¤ç§æ–¹æ¡ˆ**ï¼š

1. **å›ºå®šä½ç½®ç¼–ç **ï¼ˆæ­£å¼¦/ä½™å¼¦ï¼‰ï¼š

$$
\text{PE}(pos, 2i) = \sin\left( \frac{pos}{10000^{2i/D}} \right)
$$

$$
\text{PE}(pos, 2i+1) = \cos\left( \frac{pos}{10000^{2i/D}} \right)
$$

2. **å¯å­¦ä¹ ä½ç½®ç¼–ç **ï¼ˆTransUNeté‡‡ç”¨ï¼‰ï¼š

$$
X_{\text{pos}} = X + E_{\text{pos}}
$$

å…¶ä¸­ \( E_{\text{pos}} \in \mathbb{R}^{N \times D} \) æ˜¯å¯å­¦ä¹ å‚æ•°ã€‚

#### 3. Transformer Encoder

```python
class TransformerEncoder(nn.Module):
    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4, depth=12):
        super().__init__()
        self.layers = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, mlp_ratio)
            for _ in range(depth)
        ])
    
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x


class TransformerBlock(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, num_heads)
        
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * mlp_ratio),
            nn.GELU(),
            nn.Linear(dim * mlp_ratio, dim)
        )
    
    def forward(self, x):
        # Multi-Head Self-Attention
        x_norm = self.norm1(x)
        attn_out, _ = self.attn(x_norm, x_norm, x_norm)
        x = x + attn_out  # æ®‹å·®è¿æ¥
        
        # Feed-Forward Network
        x = x + self.mlp(self.norm2(x))
        
        return x
```

#### 4. å®Œæ•´TransUNetå®ç°

```python
class TransUNet(nn.Module):
    def __init__(self, 
                 img_size=224, 
                 in_channels=3,
                 num_classes=2,
                 embed_dim=768,
                 num_heads=12,
                 depth=12):
        super().__init__()
        
        ### CNNç¼–ç å™¨ ###
        self.enc1 = nn.Sequential(
            nn.Conv2d(in_channels, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )  # HÃ—WÃ—64
        
        self.enc2 = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )  # H/2Ã—W/2Ã—128
        
        self.enc3 = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )  # H/4Ã—W/4Ã—256
        
        self.enc4 = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(256, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )  # H/8Ã—W/8Ã—512
        
        ### Transformer ###
        num_patches = (img_size // 8) ** 2
        
        self.patch_embed = PatchEmbedding(512, embed_dim, patch_size=1)
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
        
        self.transformer = TransformerEncoder(embed_dim, num_heads, depth=depth)
        
        ### CNNè§£ç å™¨ ###
        self.dec3 = nn.Sequential(
            nn.ConvTranspose2d(embed_dim, 256, 2, stride=2),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, 3, padding=1),  # 512 = 256(skip) + 256(up)
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )  # H/4Ã—W/4Ã—256
        
        self.dec2 = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 2, stride=2),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )  # H/2Ã—W/2Ã—128
        
        self.dec1 = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 2, stride=2),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )  # HÃ—WÃ—64
        
        self.out = nn.Conv2d(64, num_classes, 1)
    
    def forward(self, x):
        # CNNç¼–ç å™¨
        e1 = self.enc1(x)  # (B, 64, H, W)
        e2 = self.enc2(e1)  # (B, 128, H/2, W/2)
        e3 = self.enc3(e2)  # (B, 256, H/4, W/4)
        e4 = self.enc4(e3)  # (B, 512, H/8, W/8)
        
        # Transformer
        B, C, H, W = e4.shape
        x_tokens, (h, w) = self.patch_embed(e4)  # (B, H*W, 768)
        x_tokens = x_tokens + self.pos_embed  # ä½ç½®ç¼–ç 
        
        x_trans = self.transformer(x_tokens)  # (B, H*W, 768)
        
        # Reshapeå›2Dç‰¹å¾å›¾
        x_trans = x_trans.transpose(1, 2).view(B, -1, h, w)  # (B, 768, H/8, W/8)
        
        # CNNè§£ç å™¨ + Skip Connections
        d3 = self.dec3(x_trans)  # ä¸Šé‡‡æ ·
        d3 = torch.cat([d3, e3], dim=1)  # Skipè¿æ¥
        d3 = self.dec3[2:](d3)  # å·ç§¯èåˆ
        
        d2 = self.dec2(d3)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2[2:](d2)
        
        d1 = self.dec1(d2)
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1[2:](d1)
        
        # è¾“å‡º
        out = self.out(d1)  # (B, num_classes, H, W)
        return out
```

---

## ğŸ“Š å®éªŒç»“æœ

### æ•°æ®é›†

| æ•°æ®é›† | ä»»åŠ¡ | æ¨¡æ€ | æ ·æœ¬æ•° | æŒ‘æˆ˜ |
|--------|------|------|--------|------|
| **Synapse multi-organ** | å¤šå™¨å®˜åˆ†å‰²ï¼ˆ8ç±»ï¼‰ | CT | 30ä¾‹ï¼Œ18è®­ç»ƒ/12æµ‹è¯• | å™¨å®˜å°ºåº¦å·®å¼‚å¤§ |
| **ACDC** | å¿ƒè„åˆ†å‰² | MRI | 100ä¾‹ï¼Œ70è®­ç»ƒ/30æµ‹è¯• | è¾¹ç•Œæ¨¡ç³Š |

### æ€§èƒ½å¯¹æ¯”

#### Synapseæ•°æ®é›†ï¼ˆ8å™¨å®˜åˆ†å‰²ï¼‰

| æ–¹æ³• | å¹³å‡Dice | å¹³å‡HD95 | å‚æ•°é‡ |
|------|---------|---------|---------|
| UNet | 76.85 | 39.70 | 31M |
| Attention UNet | 77.77 | 36.02 | 35M |
| UNet++ | 78.32 | 34.16 | 45M |
| **TransUNet** | **81.87** | **28.78** | **105M** |

**å„å™¨å®˜Dice**ï¼š

| å™¨å®˜ | UNet | Attention UNet | UNet++ | **TransUNet** |
|------|------|---------------|--------|--------------|
| ä¸»åŠ¨è„‰ | 87.23 | 88.61 | 89.07 | **90.75** |
| èƒ†å›Š | 68.60 | 70.30 | 71.15 | **77.42** |
| å·¦è‚¾ | 84.18 | 84.66 | 85.92 | **88.31** |
| å³è‚¾ | 77.98 | 79.24 | 80.11 | **84.22** |
| è‚è„ | 93.88 | 93.57 | 94.05 | **94.99** |
| èƒ°è…º | 56.45 | 60.63 | 61.21 | **70.84** |
| è„¾è„ | 88.11 | 88.59 | 89.16 | **92.13** |
| èƒƒ | 75.62 | 76.51 | 76.88 | **82.30** |

**å…³é”®è§‚å¯Ÿ**ï¼š
- âœ… **å°å™¨å®˜æå‡æ˜æ˜¾**ï¼šèƒ†å›Šï¼ˆ+6.3%ï¼‰ã€èƒ°è…ºï¼ˆ+9.6%ï¼‰
- âœ… **å¤§å™¨å®˜ä¹Ÿæœ‰æå‡**ï¼šä¸»åŠ¨è„‰ï¼ˆ+1.7%ï¼‰ã€è„¾è„ï¼ˆ+4.0%ï¼‰
- âœ… **å¹³å‡HD95å¤§å¹…ä¸‹é™**ï¼š39.70 â†’ 28.78ï¼ˆ-27.5%ï¼‰

#### ACDCæ•°æ®é›†ï¼ˆå¿ƒè„åˆ†å‰²ï¼‰

| æ–¹æ³• | RV Dice | Myo Dice | LV Dice | å¹³å‡Dice |
|------|---------|----------|---------|---------|
| UNet | 87.55 | 80.83 | 94.05 | 87.48 |
| Attention UNet | 88.39 | 81.24 | 94.56 | 88.06 |
| **TransUNet** | **89.71** | **84.53** | **95.73** | **90.00** |

**æå‡**ï¼š+2.5% Dice vs. UNet

---

## ğŸ’¡ TransUNetçš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜

### âœ… ä¼˜åŠ¿

#### 1. å…¨å±€å»ºæ¨¡èƒ½åŠ›

**ç¤ºä¾‹ï¼šä¸»åŠ¨è„‰åˆ†å‰²**

```
æ ‡å‡†UNetï¼š
- åªèƒ½é€šè¿‡å¤šå±‚å·ç§¯é€æ­¥æ‰©å¤§æ„Ÿå—é‡
- éš¾ä»¥æ•æ‰ä¸»åŠ¨è„‰ä»å¿ƒè„åˆ°è…¹éƒ¨çš„è¿ç»­æ€§

TransUNetï¼š
- Transformerç›´æ¥å»ºæ¨¡å…¨å›¾ä¾èµ–
- ç†è§£ä¸»åŠ¨è„‰çš„å®Œæ•´èµ°å‘
- Dice: 87.23% â†’ 90.75%ï¼ˆ+3.5%ï¼‰
```

**å¯è§†åŒ–æ³¨æ„åŠ›å›¾**ï¼š

```python
# æå–Transformerçš„æ³¨æ„åŠ›æƒé‡
def visualize_attention(model, image):
    with torch.no_grad():
        _ = model(image)
        # è·å–æœ€åä¸€å±‚Transformerçš„æ³¨æ„åŠ›
        attn_weights = model.transformer.layers[-1].attn.attn_weights
        # attn_weights: (B, num_heads, N, N)
        
    # å¹³å‡æ‰€æœ‰head
    attn_avg = attn_weights.mean(dim=1)  # (B, N, N)
    
    # å¯è§†åŒ–ç¬¬ä¸€ä¸ªtokenå…³æ³¨çš„åŒºåŸŸ
    attn_map = attn_avg[0, 0, :].view(h, w)
    
    plt.imshow(attn_map.cpu(), cmap='hot')
    plt.title('Attention Map from Token 1')
    plt.show()
```

**å…¸å‹æ¨¡å¼**ï¼š
- æµ…å±‚headï¼šå…³æ³¨å±€éƒ¨é‚»åŸŸï¼ˆç±»ä¼¼å·ç§¯ï¼‰
- æ·±å±‚headï¼šå…³æ³¨è¿œè·ç¦»ä¾èµ–ï¼ˆç‹¬ç‰¹ä¼˜åŠ¿ï¼‰

#### 2. å¤„ç†å¤šå°ºåº¦ç›®æ ‡

```
åœºæ™¯ï¼šåŒæ—¶åˆ†å‰²å¤§å™¨å®˜ï¼ˆè‚è„94%ï¼‰å’Œå°å™¨å®˜ï¼ˆèƒ†å›Š2%ï¼‰

CNNçš„å›°å¢ƒï¼š
- æµ…å±‚ç‰¹å¾ï¼šé«˜åˆ†è¾¨ç‡ï¼Œé€‚åˆå°ç›®æ ‡
- æ·±å±‚ç‰¹å¾ï¼šä½åˆ†è¾¨ç‡ï¼Œé€‚åˆå¤§ç›®æ ‡
- Skipè¿æ¥èåˆæ•ˆæœæœ‰é™

Transformerçš„ä¼˜åŠ¿ï¼š
- è‡ªæ³¨æ„åŠ›åœ¨ä»»æ„å°ºåº¦éƒ½èƒ½æ„ŸçŸ¥å…¨å›¾
- å°å™¨å®˜ï¼ˆèƒ†å›Šï¼‰å¯ä»¥ç›´æ¥"çœ‹åˆ°"å¤§å™¨å®˜ï¼ˆè‚è„ï¼‰ä½œä¸ºå‚è€ƒ
- èƒ†å›ŠDice: 68.60% â†’ 77.42%ï¼ˆ+8.8%ï¼‰
```

#### 3. æ›´å°‘çš„å½’çº³åç½®

**CNNçš„å½’çº³åç½®**ï¼š
- å±€éƒ¨æ€§ï¼ˆlocalityï¼‰ï¼šå·ç§¯åªçœ‹å±€éƒ¨
- å¹³ç§»ç­‰å˜æ€§ï¼ˆtranslation equivarianceï¼‰ï¼šç‰¹å¾å›¾å¹³ç§»ï¼Œè¾“å‡ºä¹Ÿå¹³ç§»

**Transformerçš„ä¼˜åŠ¿**ï¼š
- æ›´å°‘å‡è®¾ï¼Œæ›´å¼ºè¡¨è¾¾èƒ½åŠ›
- é€šè¿‡å¤§é‡æ•°æ®å­¦ä¹ æœ€ä¼˜ç»“æ„
- æ›´å¥½çš„æ³›åŒ–æ€§

### âŒ æŒ‘æˆ˜

#### 1. è®¡ç®—å¤æ‚åº¦é«˜

**è‡ªæ³¨æ„åŠ›çš„å¤æ‚åº¦**ï¼š\( O(N^2 D) \)

```
è®¡ç®—é‡åˆ†æï¼š

è¾“å…¥å›¾åƒï¼š224Ã—224
ä¸‹é‡‡æ ·åˆ°ï¼šH/8 Ã— W/8 = 28Ã—28 = 784 tokens

æ ‡å‡†UNetï¼š
- å‚æ•°ï¼š31M
- FLOPsï¼šçº¦50 GFLOPs

TransUNetï¼š
- å‚æ•°ï¼š105Mï¼ˆ+3.4Ã—ï¼‰
- FLOPsï¼šçº¦200 GFLOPsï¼ˆ+4Ã—ï¼‰
- æ³¨æ„åŠ›çŸ©é˜µï¼š784Ã—784 = 614,656ï¼ˆæ¯å±‚æ¯ä¸ªheadï¼‰
```

**å†…å­˜å ç”¨**ï¼š

$$
\text{Memory}_{\text{attn}} = B \times H \times N \times N \times 4 \text{ bytes}
$$

ç¤ºä¾‹ï¼šBatch=4, Heads=12, N=784

$$
\text{Memory} = 4 \times 12 \times 784 \times 784 \times 4 \approx 118 \text{ MBï¼ˆä»…æ³¨æ„åŠ›çŸ©é˜µï¼‰}
$$

#### 2. éœ€è¦å¤§é‡æ•°æ®

**Transformerç¼ºä¹å½’çº³åç½®ï¼Œéœ€è¦æ›´å¤šæ•°æ®**ï¼š

```
ImageNeté¢„è®­ç»ƒï¼ˆ1.2Må›¾åƒï¼‰ï¼š
- TransUNetæ€§èƒ½æ˜¾è‘—æå‡
- æ— é¢„è®­ç»ƒæ—¶æ€§èƒ½ä¸‹é™5-7% Dice

å°æ•°æ®é›†ï¼ˆ<100ä¾‹ï¼‰ï¼š
- Transformerå¯èƒ½è¿‡æ‹Ÿåˆ
- UNetä»ç„¶æ›´ç¨³å®š
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨ImageNeté¢„è®­ç»ƒçš„ViT
- æ•°æ®å¢å¼ºï¼ˆæ—‹è½¬ã€ç¿»è½¬ã€å¼¹æ€§å˜å½¢ï¼‰
- æ­£åˆ™åŒ–ï¼ˆDropoutã€StochasticDepthï¼‰

#### 3. é«˜åˆ†è¾¨ç‡å›¾åƒå›°éš¾

**é—®é¢˜**ï¼šåŒ»å­¦å›¾åƒé€šå¸¸å¾ˆå¤§ï¼ˆ512Ã—512æˆ–æ›´å¤§ï¼‰

```
512Ã—512å›¾åƒï¼š
ä¸‹é‡‡æ ·åˆ°H/8 Ã— W/8 = 64Ã—64 = 4096 tokens

æ³¨æ„åŠ›å¤æ‚åº¦ï¼š
O(4096^2) â‰ˆ 16.8Mæ¬¡ä¹˜æ³•ï¼ˆæ¯å±‚æ¯ä¸ªheadï¼‰

å†…å­˜çˆ†ç‚¸ï¼š
Batch=2, Heads=12, N=4096
â†’ å†…å­˜éœ€æ±‚ï¼šçº¦3GBï¼ˆä»…æ³¨æ„åŠ›ï¼‰
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ›´aggressiveçš„ä¸‹é‡‡æ ·ï¼ˆH/16ï¼‰
- Patch mergingï¼ˆå¦‚Swin Transformerï¼‰
- Window-based attentionï¼ˆå±€éƒ¨æ³¨æ„åŠ›ï¼‰

---

## ğŸ”¬ TransUNetçš„å˜ç§

### 1. MedTï¼ˆMedical Transformerï¼‰

**æ”¹è¿›**ï¼šGated Axial Attentionï¼ˆé—¨æ§è½´å‘æ³¨æ„åŠ›ï¼‰

```python
class GatedAxialAttention(nn.Module):
    """
    å°†2Dæ³¨æ„åŠ›åˆ†è§£ä¸ºè¡Œæ³¨æ„åŠ›+åˆ—æ³¨æ„åŠ›
    å¤æ‚åº¦ï¼šO(HÃ—WÃ—(H+W)) vs. O(H^2Ã—W^2)
    """
    def __init__(self, dim):
        super().__init__()
        self.row_attn = AxialAttention(dim, axis=0)
        self.col_attn = AxialAttention(dim, axis=1)
        self.gate = nn.Parameter(torch.ones(1))
    
    def forward(self, x):
        # x: (B, C, H, W)
        x_row = self.row_attn(x)
        x_col = self.col_attn(x)
        return self.gate * x_row + (1 - self.gate) * x_col
```

**ä¼˜åŠ¿**ï¼š
- è®¡ç®—é‡é™ä½ï¼š\( O(N^2) \rightarrow O(N\sqrt{N}) \)
- é€‚åˆé«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒ

### 2. UNETR

**æ”¹è¿›**ï¼šçº¯Transformerç¼–ç å™¨ï¼Œæ— CNN

```
è¾“å…¥  â†’  Patch Embedding  â†’  Transformer Ã— 12
                                     â†“
                        æ¯3å±‚æå–ç‰¹å¾ä½œä¸ºskip
                                     â†“
                              CNNè§£ç å™¨
```

**ç‰¹ç‚¹**ï¼š
- æ›´å½»åº•çš„Transformerè®¾è®¡
- æ€§èƒ½ä¸TransUNetç›¸å½“
- å‚æ•°é‡ç•¥å°

### 3. CoTr (Contextual Transformer)

**æ”¹è¿›**ï¼šå¤šå°ºåº¦Transformer

```
ç¼–ç å™¨ï¼š
â”œâ”€ Transformer @ H/8ï¼ˆé«˜åˆ†è¾¨ç‡ï¼‰
â”œâ”€ Transformer @ H/16ï¼ˆä¸­åˆ†è¾¨ç‡ï¼‰
â””â”€ Transformer @ H/32ï¼ˆä½åˆ†è¾¨ç‡ï¼‰

è§£ç å™¨èåˆå¤šå°ºåº¦ç‰¹å¾
```

---

## ğŸ“ è®­ç»ƒæŠ€å·§

### 1. é¢„è®­ç»ƒç­–ç•¥

```python
# ä½¿ç”¨ImageNeté¢„è®­ç»ƒçš„ViTæƒé‡
def load_pretrained_vit(model, vit_name='vit_base_patch16_224'):
    import timm
    
    # åŠ è½½é¢„è®­ç»ƒViT
    pretrained_vit = timm.create_model(vit_name, pretrained=True)
    
    # æå–Transformeræƒé‡
    model_dict = model.state_dict()
    pretrained_dict = pretrained_vit.state_dict()
    
    # ä»…åŠ è½½Transformeréƒ¨åˆ†
    pretrained_dict = {
        k: v for k, v in pretrained_dict.items() 
        if 'blocks' in k and k in model_dict
    }
    
    model_dict.update(pretrained_dict)
    model.load_state_dict(model_dict)
    
    print(f"Loaded {len(pretrained_dict)} layers from {vit_name}")
```

### 2. ä¸¤é˜¶æ®µè®­ç»ƒ

```python
# é˜¶æ®µ1ï¼šå†»ç»“Transformerï¼Œè®­ç»ƒCNNéƒ¨åˆ†
for epoch in range(50):
    # å†»ç»“Transformer
    for name, param in model.named_parameters():
        if 'transformer' in name:
            param.requires_grad = False
    
    train_epoch(model, train_loader)

# é˜¶æ®µ2ï¼šè§£å†»Transformerï¼Œfine-tuneå…¨ç½‘ç»œ
for epoch in range(50, 150):
    # è§£å†»Transformer
    for param in model.parameters():
        param.requires_grad = True
    
    # ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
    train_epoch(model, train_loader)
```

### 3. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for images, masks in train_loader:
    optimizer.zero_grad()
    
    # è‡ªåŠ¨æ··åˆç²¾åº¦
    with autocast():
        outputs = model(images)
        loss = criterion(outputs, masks)
    
    # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**ä¼˜åŠ¿**ï¼š
- å‡å°‘50%å†…å­˜å ç”¨
- åŠ é€Ÿ20-30%è®­ç»ƒ
- ç²¾åº¦å‡ ä¹æ— æŸå¤±

---

## ğŸ“– æ€»ç»“

### TransUNetçš„æ ¸å¿ƒè´¡çŒ®

1. **é¦–æ¬¡å°†TransformeræˆåŠŸåº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²**
   - è¯æ˜å…¨å±€å»ºæ¨¡å¯¹åŒ»å­¦å›¾åƒçš„é‡è¦æ€§
   - Diceæå‡ï¼š76.85% â†’ 81.87%ï¼ˆ+5%ï¼‰

2. **æ··åˆæ¶æ„è®¾è®¡**
   - CNNæå–å±€éƒ¨ç‰¹å¾
   - Transformerå»ºæ¨¡å…¨å±€ä¾èµ–
   - ä¸¤è€…ä¼˜åŠ¿äº’è¡¥

3. **å¼€å¯åŒ»å­¦åˆ†å‰²Transformeræ—¶ä»£**
   - åç»­æ¶Œç°å¤§é‡Transformeråˆ†å‰²ç½‘ç»œ
   - æˆä¸ºæ–°èŒƒå¼çš„åŸºçŸ³

### é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ˜¯å¦é€‚åˆ | åŸå›  |
|------|---------|------|
| å¤§å™¨å®˜ï¼ˆè‚è„ã€è‚ºï¼‰ | âœ… | å…¨å±€ç»“æ„å»ºæ¨¡ |
| å°å™¨å®˜ï¼ˆèƒ°è…ºã€èƒ†å›Šï¼‰ | âœ…âœ… | é•¿è·ç¦»ä¾èµ– |
| å¤šç±»åˆ«åˆ†å‰² | âœ…âœ… | å…¨å±€ä¸Šä¸‹æ–‡ |
| å°æ•°æ®é›†ï¼ˆ<50ä¾‹ï¼‰ | âŒ | éœ€è¦é¢„è®­ç»ƒ |
| å®æ—¶åº”ç”¨ | âŒ | è®¡ç®—é‡å¤§ |
| é«˜åˆ†è¾¨ç‡ï¼ˆ>512ï¼‰ | âš ï¸ | éœ€è¦ä¼˜åŒ– |

**ä¸‹ä¸€ç¯‡é¢„å‘Š**ï¼š[Swin-UNetï¼šå±‚çº§åŒ–Transformer](/2025/02/25/swin-unet-hierarchical-transformer/) - æ¢ç´¢å¦‚ä½•é€šè¿‡shifted windowså’Œå±‚çº§ç»“æ„å…‹æœTransUNetçš„è®¡ç®—ç“¶é¢ˆã€‚

---

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡
1. [TransUNet] Chen, J., et al. (2021). TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation. *arXiv*.
2. [Vision Transformer] Dosovitskiy, A., et al. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *ICLR*.
3. [Attention is All You Need] Vaswani, A., et al. (2017). Attention is All You Need. *NeurIPS*.

### ä»£ç å®ç°
- [TransUNetå®˜æ–¹](https://github.com/Beckschen/TransUNet) - PyTorchå®ç°
- [UNETRå®˜æ–¹](https://github.com/Project-MONAI/research-contributions/tree/main/UNETR) - MONAIå®ç°
- [Medical Transformeråº“](https://github.com/jeya-maria-jose/Medical-Transformer) - å¤šç§åŒ»å­¦Transformer

### æ•°æ®é›†
- [Synapse Multi-organ](https://www.synapse.org/#!Synapse:syn3193805/wiki/217789) - å¤šå™¨å®˜CTåˆ†å‰²
- [ACDC](https://www.creatis.insa-lyon.fr/Challenge/acdc/) - å¿ƒè„MRIåˆ†å‰²

---

## ğŸ”— ç³»åˆ—æ–‡ç« å¯¼èˆª

1. [FCNä¸UNetï¼šåŒ»å­¦åˆ†å‰²çš„å¥ åŸºä¹‹ä½œ](/2025/02/01/fcn-unet-foundation/)
2. [V-Netï¼š3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„çªç ´](/2025/02/05/vnet-3d-segmentation/)
3. [Attention UNetï¼šæ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥](/2025/02/10/attention-unet/)
4. [UNet++/UNet 3+ï¼šå¯†é›†è¿æ¥çš„åŠ›é‡](/2025/02/15/unet-plus-series/)
5. ğŸ“ **TransUNetï¼šCNNä¸Transformerçš„èåˆ**ï¼ˆæœ¬æ–‡ï¼‰
6. [Swin-UNetï¼šå±‚çº§åŒ–Transformer](/2025/02/25/swin-unet-hierarchical-transformer/)
7. [SAMä¸MedSAMï¼šåŸºç¡€æ¨¡å‹çš„åŒ»å­¦åº”ç”¨](/2025/03/05/sam-segment-anything/)
8. [nnU-Netï¼šè‡ªé€‚åº”åŒ»å­¦åˆ†å‰²æ¡†æ¶](/2025/03/15/nnunet-self-configuring-framework/)

---

*æœ¬æ–‡æ·±å…¥æ¢è®¨äº†TransUNetå¦‚ä½•å°†Transformerçš„å…¨å±€å»ºæ¨¡èƒ½åŠ›å¼•å…¥åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œå¼€åˆ›äº†æ··åˆæ¶æ„çš„æ–°èŒƒå¼ã€‚ä¸‹ä¸€ç¯‡å°†ä»‹ç»Swin-UNetå¦‚ä½•é€šè¿‡å±‚çº§ç»“æ„å’Œshifted windowsä¼˜åŒ–Transformeræ•ˆç‡ã€‚*

