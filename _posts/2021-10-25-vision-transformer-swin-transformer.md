---
layout: post
title: "Vision Transformerï¼šTransformeråœ¨è®¡ç®—æœºè§†è§‰çš„é©å‘½"
date: 2021-10-25 10:00:00 +0800
categories: [æ·±åº¦å­¦ä¹ , Transformer]
tags: [Transformer, è®¡ç®—æœºè§†è§‰, PyTorch]
excerpt: "æ·±å…¥è§£æVision Transformerå’ŒSwin Transformerã€‚æ¢ç´¢Transformerå¦‚ä½•ä»NLPè·¨ç•Œåˆ°CVï¼Œä»¥åŠå¦‚ä½•é€šè¿‡çª—å£æ³¨æ„åŠ›æœºåˆ¶å®ç°é«˜æ•ˆçš„å›¾åƒå¤„ç†ã€‚"
---

# Vision Transformerï¼šTransformeråœ¨è®¡ç®—æœºè§†è§‰çš„é©å‘½

## å¼•è¨€

Transformeråœ¨NLPé¢†åŸŸå–å¾—å·¨å¤§æˆåŠŸåï¼Œç ”ç©¶è€…è‡ªç„¶ä¼šæ€è€ƒï¼š**èƒ½å¦å°†Transformeråº”ç”¨åˆ°è®¡ç®—æœºè§†è§‰ï¼Ÿ**

2020å¹´ï¼ŒGoogleæå‡ºçš„**Vision Transformer (ViT)**ç»™å‡ºäº†è‚¯å®šçš„ç­”æ¡ˆï¼Œå¹¶æ€èµ·äº†CVé¢†åŸŸçš„Transformeræµªæ½®ã€‚2021å¹´ï¼Œå¾®è½¯çš„**Swin Transformer**æ›´æ˜¯å°†è¿™ä¸€æµªæ½®æ¨å‘é«˜æ½®ï¼Œè·å¾—äº†ICCV 2021æœ€ä½³è®ºæ–‡å¥–ã€‚

## 1. Vision Transformer (2020)

### åŸºæœ¬ä¿¡æ¯

* **å›¢é˜Ÿ**ï¼šGoogle Research
* **è®ºæ–‡**ï¼š[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
* **DOI**ï¼šarXiv:2010.11929
* **æ—¶é—´**ï¼š2020å¹´

### æ ¸å¿ƒæ€æƒ³

**An Image is Worth 16Ã—16 Words**ï¼šå°†å›¾åƒçœ‹ä½œåºåˆ—ï¼

![Vision Transformerç»“æ„](/assets/images/deep-learning/VisionTransformer.png)

### æ¶æ„è®¾è®¡

#### 1. å›¾åƒåˆ†å—ï¼ˆPatch Embeddingï¼‰

```python
class PatchEmbedding(nn.Module):
    """å°†å›¾åƒåˆ‡åˆ†æˆpatcheså¹¶åµŒå…¥"""
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super(PatchEmbedding, self).__init__()
        
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        
        # ä½¿ç”¨å·ç§¯å®ç°patch embedding
        self.proj = nn.Conv2d(in_channels, embed_dim, 
                             kernel_size=patch_size, stride=patch_size)
    
    def forward(self, x):
        # x: (batch, 3, 224, 224)
        x = self.proj(x)  # (batch, embed_dim, 14, 14)
        x = x.flatten(2)  # (batch, embed_dim, 196)
        x = x.transpose(1, 2)  # (batch, 196, embed_dim)
        return x
```

**å…³é”®æ­¥éª¤**ï¼š
1. 224Ã—224å›¾åƒ â†’ åˆ‡åˆ†æˆ14Ã—14=196ä¸ª16Ã—16çš„patches
2. æ¯ä¸ªpatchå±•å¹³æˆ384ç»´å‘é‡
3. çº¿æ€§æŠ•å½±åˆ°768ç»´ï¼ˆembed_dimï¼‰

#### 2. ä½ç½®ç¼–ç 

```python
class ViT(nn.Module):
    def __init__(self, img_size=224, patch_size=16, num_classes=1000, 
                 embed_dim=768, depth=12, num_heads=12):
        super(ViT, self).__init__()
        
        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)
        num_patches = self.patch_embed.num_patches
        
        # Class tokenï¼šå¯å­¦ä¹ çš„åˆ†ç±»token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # Position embeddingï¼šå¯å­¦ä¹ çš„ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        
        # Transformer Encoder
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=embed_dim*4),
            num_layers=depth
        )
        
        # Classification head
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)
    
    def forward(self, x):
        batch_size = x.shape[0]
        
        # Patch embedding
        x = self.patch_embed(x)  # (batch, 196, 768)
        
        # æ·»åŠ class token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch, 1, 768)
        x = torch.cat([cls_tokens, x], dim=1)  # (batch, 197, 768)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed
        
        # Transformer
        x = self.transformer(x)
        
        # åˆ†ç±»
        x = self.norm(x)
        cls_token_final = x[:, 0]  # åªå–class token
        x = self.head(cls_token_final)
        
        return x
```

### æ ¸å¿ƒç»„ä»¶

#### Class Token

**æ€æƒ³**ï¼šå€Ÿé‰´BERTçš„[CLS] token

* åœ¨åºåˆ—å¼€å¤´æ·»åŠ ä¸€ä¸ªå¯å­¦ä¹ çš„token
* ç”¨äºèšåˆæ•´ä¸ªåºåˆ—çš„ä¿¡æ¯
* æœ€åç”¨äºåˆ†ç±»

#### Position Embedding

**ViTä½¿ç”¨å¯å­¦ä¹ çš„ä½ç½®ç¼–ç **ï¼š

```python
# 1Dä½ç½®ç¼–ç ï¼ˆæ¯ä¸ªpatchä¸€ä¸ªï¼‰
self.pos_embed = nn.Parameter(torch.zeros(1, 197, 768))
```

**ä¸Transformerçš„åŒºåˆ«**ï¼š
* Transformerï¼šä½¿ç”¨å›ºå®šçš„sin/cosç¼–ç 
* ViTï¼šä½¿ç”¨å¯å­¦ä¹ çš„ç¼–ç 

### ViTçš„é…ç½®

| æ¨¡å‹ | å±‚æ•° | éšè—ç»´åº¦ | MLPç»´åº¦ | å¤´æ•° | å‚æ•°é‡ |
|------|------|---------|---------|------|--------|
| ViT-Base | 12 | 768 | 3072 | 12 | 86M |
| ViT-Large | 24 | 1024 | 4096 | 16 | 307M |
| ViT-Huge | 32 | 1280 | 5120 | 16 | 632M |

### æ ¸å¿ƒå‘ç°

**ViTè®ºæ–‡çš„æ ¸å¿ƒè§‚ç‚¹**ï¼š

> å½“æ‹¥æœ‰è¶³å¤Ÿå¤šçš„æ•°æ®è¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼ŒViTçš„è¡¨ç°ä¼šè¶…è¿‡CNNã€‚

#### æ•°æ®éœ€æ±‚

| æ•°æ®é›†è§„æ¨¡ | ViTè¡¨ç° | ResNetè¡¨ç° |
|-----------|---------|-----------|
| ImageNet (1.2M) | è¾ƒå·® | âœ… å¥½ |
| ImageNet-21K (14M) | ç›¸å½“ | âœ… å¥½ |
| JFT-300M (300M) | âœ… **æ›´å¥½** | å¥½ |

**ç»“è®º**ï¼šViTéœ€è¦å¤§è§„æ¨¡é¢„è®­ç»ƒï¼

### å½’çº³åç½®ï¼ˆInductive Biasï¼‰

**CNNçš„å½’çº³åç½®**ï¼š
* **å±€éƒ¨æ€§ï¼ˆLocalityï¼‰**ï¼šç›¸é‚»åƒç´ ç›¸å…³
* **å¹³ç§»ä¸å˜æ€§ï¼ˆTranslation Equivarianceï¼‰**ï¼šå·ç§¯æ ¸å…±äº«

**ViTçš„å½’çº³åç½®**ï¼š
* **æ›´å°‘çš„å½’çº³åç½®**
* æ›´ä¾èµ–æ•°æ®æ¥å­¦ä¹ 
* åœ¨å¤§æ•°æ®ä¸‹æœ‰ä¼˜åŠ¿

### æ¨¡å‹å¤ç°

* **ä»£ç åœ°å€**ï¼š[GitHub - DeepLearning/model_classification/VisionTransformer](https://github.com/YangCazz/DeepLearning/tree/master/model_classification/VisionTransformer)

## 2. Swin Transformer (2021)

### åŸºæœ¬ä¿¡æ¯

* **å›¢é˜Ÿ**ï¼šMicrosoft Research Asia
* **è®ºæ–‡**ï¼š[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)
* **DOI**ï¼šarXiv:2103.14030
* **æ—¶é—´**ï¼š2021å¹´
* **è£èª‰**ï¼šICCV 2021æœ€ä½³è®ºæ–‡

### æ ¸å¿ƒé—®é¢˜

**ViTçš„é—®é¢˜**ï¼š
1. **è®¡ç®—å¤æ‚åº¦é«˜**ï¼šå…¨å±€æ³¨æ„åŠ›å¤æ‚åº¦O(nÂ²)
2. **ç‰¹å¾å•ä¸€**ï¼šåªæœ‰16å€ä¸‹é‡‡æ ·ï¼Œä¸é€‚åˆå¯†é›†é¢„æµ‹
3. **ç¼ºå°‘å±‚æ¬¡åŒ–ç»“æ„**ï¼šä¸åƒCNNæœ‰å¤šå°ºåº¦ç‰¹å¾

### æ ¸å¿ƒåˆ›æ–°

![Swin Transformerç»“æ„](/assets/images/deep-learning/SwinTransformer.png)

#### 1. çª—å£å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆW-MSAï¼‰

![Swin Feature Maps](/assets/images/deep-learning/SwinTransformer_Feature_Maps.png)

**æ€æƒ³**ï¼šå°†å›¾åƒåˆ’åˆ†æˆä¸é‡å çš„çª—å£ï¼Œåªåœ¨çª—å£å†…è®¡ç®—æ³¨æ„åŠ›ã€‚

```python
def window_partition(x, window_size):
    """
    å°†ç‰¹å¾å›¾åˆ’åˆ†æˆçª—å£
    x: (B, H, W, C)
    window_size: çª—å£å¤§å°M
    è¿”å›: (B*num_windows, M, M, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous()
    windows = windows.view(-1, window_size, window_size, C)
    return windows

def window_reverse(windows, window_size, H, W):
    """
    å°†çª—å£è¿˜åŸæˆç‰¹å¾å›¾
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x
```

**å¤æ‚åº¦åˆ†æ**ï¼š

å…¨å±€æ³¨æ„åŠ›ï¼ˆViTï¼‰ï¼š
$$
\text{FLOPs} = 4hwC^2 + 2(hw)^2C
$$

çª—å£æ³¨æ„åŠ›ï¼ˆSwinï¼‰ï¼š
$$
\text{FLOPs} = 4hwC^2 + 2hwM^2C
$$

**å·®è·**ï¼š\(2(hw)^2C - 2hwM^2C\)

å½“h=w=56, M=7æ—¶ï¼ŒèŠ‚çœ**çº¦49å€**ï¼

#### 2. æ»‘åŠ¨çª—å£å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆSW-MSAï¼‰

![SW-MSA](/assets/images/deep-learning/SwinTransformer_SW_MSA.png)

**é—®é¢˜**ï¼šW-MSAéš”ç»äº†çª—å£ä¹‹é—´çš„ä¿¡æ¯äº¤æµã€‚

**è§£å†³**ï¼šæ»‘åŠ¨çª—å£ï¼

**å¾ªç¯ç§»ä½æœºåˆ¶**ï¼š

![Cyclic Shift](/assets/images/deep-learning/SwinTransformer_CyclicShift.png)

```python
class SwinTransformerBlock(nn.Module):
    def __init__(self, dim, num_heads, window_size=7, shift_size=0):
        super(SwinTransformerBlock, self).__init__()
        
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        
        self.norm1 = nn.LayerNorm(dim)
        self.attn = WindowAttention(dim, window_size, num_heads)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = MLP(dim, dim * 4)
    
    def forward(self, x):
        H, W = self.H, self.W
        B, L, C = x.shape
        
        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)
        
        # å¾ªç¯ç§»ä½
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x
        
        # çª—å£åˆ’åˆ†
        x_windows = window_partition(shifted_x, self.window_size)
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)
        
        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows)
        
        # çª—å£è¿˜åŸ
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)
        
        # åå‘å¾ªç¯ç§»ä½
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        
        x = x.view(B, H * W, C)
        
        # æ®‹å·®è¿æ¥
        x = shortcut + x
        x = x + self.mlp(self.norm2(x))
        
        return x
```

#### 3. Patch Merging

![Patch Merging](/assets/images/deep-learning/SwinTransformer_PatchMerging.png)

**æ€æƒ³**ï¼šæ„å»ºå±‚æ¬¡åŒ–ç‰¹å¾ï¼Œç±»ä¼¼CNNçš„ä¸‹é‡‡æ ·ã€‚

```python
class PatchMerging(nn.Module):
    """ä¸‹é‡‡æ ·æ¨¡å—ï¼š2x2é‚»åŸŸåˆå¹¶"""
    def __init__(self, dim):
        super(PatchMerging, self).__init__()
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = nn.LayerNorm(4 * dim)
    
    def forward(self, x, H, W):
        """
        x: (B, H*W, C)
        """
        B, L, C = x.shape
        x = x.view(B, H, W, C)
        
        # 2x2é‚»åŸŸé‡‡æ ·
        x0 = x[:, 0::2, 0::2, :]  # (B, H/2, W/2, C)
        x1 = x[:, 1::2, 0::2, :]
        x2 = x[:, 0::2, 1::2, :]
        x3 = x[:, 1::2, 1::2, :]
        
        x = torch.cat([x0, x1, x2, x3], dim=-1)  # (B, H/2, W/2, 4C)
        x = x.view(B, -1, 4 * C)
        
        x = self.norm(x)
        x = self.reduction(x)  # (B, H/2*W/2, 2C)
        
        return x
```

### Swin Transformerå®Œæ•´æ¶æ„

```
è¾“å…¥: 224Ã—224Ã—3

Stage 1: Patch Partition + Linear Embedding
  â†’ 56Ã—56Ã—96 (4å€ä¸‹é‡‡æ ·)
  â†’ Swin Transformer Block Ã—2

Stage 2: Patch Merging
  â†’ 28Ã—28Ã—192 (8å€ä¸‹é‡‡æ ·)
  â†’ Swin Transformer Block Ã—2

Stage 3: Patch Merging
  â†’ 14Ã—14Ã—384 (16å€ä¸‹é‡‡æ ·)
  â†’ Swin Transformer Block Ã—6

Stage 4: Patch Merging
  â†’ 7Ã—7Ã—768 (32å€ä¸‹é‡‡æ ·)
  â†’ Swin Transformer Block Ã—2

è¾“å‡º: å¤šå°ºåº¦ç‰¹å¾å›¾
```

### Swinçš„é…ç½®

| æ¨¡å‹ | C | å±‚æ•°é…ç½® | å‚æ•°é‡ | FLOPs |
|------|---|---------|--------|-------|
| Swin-T | 96 | 2,2,6,2 | 29M | 4.5G |
| Swin-S | 96 | 2,2,18,2 | 50M | 8.7G |
| Swin-B | 128 | 2,2,18,2 | 88M | 15.4G |
| Swin-L | 192 | 2,2,18,2 | 197M | 34.5G |

### ç›¸å¯¹ä½ç½®åç½®

Swinä½¿ç”¨**ç›¸å¯¹ä½ç½®åç½®ï¼ˆRelative Position Biasï¼‰**ï¼š

$$
\text{Attention}(Q, K, V) = \text{SoftMax}\left(\frac{QK^T}{\sqrt{d}} + B\right)V
$$

å…¶ä¸­Bæ˜¯å¯å­¦ä¹ çš„ç›¸å¯¹ä½ç½®åç½®çŸ©é˜µã€‚

```python
# ç›¸å¯¹ä½ç½®åç½®
self.relative_position_bias_table = nn.Parameter(
    torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)
)
```

### æ€§èƒ½å¯¹æ¯”

#### ImageNetåˆ†ç±»

| æ¨¡å‹ | å‚æ•°é‡ | FLOPs | Top-1å‡†ç¡®ç‡ |
|------|--------|-------|-----------|
| ResNet-50 | 25M | 4.1G | 79.8% |
| ViT-B | 86M | 17.6G | 81.8% |
| **Swin-T** | **29M** | **4.5G** | **81.3%** |
| **Swin-B** | **88M** | **15.4G** | **83.5%** |

#### COCOç›®æ ‡æ£€æµ‹

| Backbone | å‚æ•°é‡ | FLOPs | AP |
|----------|--------|-------|-----|
| ResNet-50 | 44M | 260G | 46.0 |
| ViT-B | 115M | 360G | 48.7 |
| **Swin-T** | **48M** | **264G** | **50.5** |

**Swinåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼**

### æ¨¡å‹å¤ç°

* **ä»£ç åœ°å€**ï¼š[GitHub - DeepLearning/model_classification/SwinTransformer](https://github.com/YangCazz/DeepLearning/tree/master/model_classification/SwinTransformer)

## ViT vs Swin Transformer

| ç»´åº¦ | ViT | Swin Transformer |
|------|-----|-----------------|
| æ³¨æ„åŠ›èŒƒå›´ | å…¨å±€ | å±€éƒ¨ï¼ˆçª—å£ï¼‰ |
| è®¡ç®—å¤æ‚åº¦ | O(nÂ²) | O(n) |
| ç‰¹å¾å±‚æ¬¡ | å•ä¸€ï¼ˆ16Ã—ï¼‰ | å¤šå°ºåº¦ï¼ˆ4Ã—,8Ã—,16Ã—,32Ã—ï¼‰ |
| æ•°æ®éœ€æ±‚ | æå¤§ | è¾ƒå¤§ |
| ä¸‹æ¸¸ä»»åŠ¡ | åˆ†ç±»ä¼˜ç§€ | æ£€æµ‹/åˆ†å‰²æ›´å¥½ |
| å½’çº³åç½® | å°‘ | é€‚ä¸­ |

## Transformer vs CNN

### CNNçš„ä¼˜åŠ¿

âœ… **å½’çº³åç½®å¼º**ï¼šå±€éƒ¨æ€§ã€å¹³ç§»ä¸å˜æ€§
âœ… **æ•°æ®æ•ˆç‡é«˜**ï¼šå°æ•°æ®ä¹Ÿèƒ½è®­ç»ƒ
âœ… **è®¡ç®—é«˜æ•ˆ**ï¼šå‚æ•°å…±äº«

### Transformerçš„ä¼˜åŠ¿

âœ… **å…¨å±€å»ºæ¨¡**ï¼šé•¿è·ç¦»ä¾èµ–
âœ… **æ‰©å±•æ€§å¥½**ï¼šæ•°æ®è¶Šå¤šè¶Šå¼º
âœ… **çµæ´»æ€§é«˜**ï¼šç»Ÿä¸€æ¶æ„

### æœªæ¥è¶‹åŠ¿

**æ··åˆæ¶æ„**ï¼šç»“åˆCNNå’ŒTransformerçš„ä¼˜åŠ¿
* ConvNeXtï¼šç°ä»£åŒ–CNN
* CoAtNetï¼šå·ç§¯+æ³¨æ„åŠ›
* CMTï¼šå·ç§¯+å¤šå¤´æ³¨æ„åŠ›

## å®è·µç»éªŒ

### 1. ä½•æ—¶ä½¿ç”¨ViT/Swinï¼Ÿ

âœ… **é€‚ç”¨åœºæ™¯**ï¼š
* å¤§è§„æ¨¡é¢„è®­ç»ƒ
* éœ€è¦å…¨å±€ä¿¡æ¯
* ä¸‹æ¸¸ä»»åŠ¡å¤šæ ·

âŒ **ä¸é€‚ç”¨**ï¼š
* æ•°æ®é‡å°
* è®¡ç®—èµ„æºå—é™
* éœ€è¦å®æ—¶æ¨ç†

### 2. é¢„è®­ç»ƒç­–ç•¥

```python
# ä½¿ç”¨ImageNet-21Ké¢„è®­ç»ƒ
model = timm.create_model('swin_base_patch4_window7_224_in22k', pretrained=True)

# åœ¨è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒ
model.head = nn.Linear(model.head.in_features, num_classes)
```

### 3. æ•°æ®å¢å¼º

Transformeréœ€è¦æ›´å¼ºçš„æ•°æ®å¢å¼ºï¼š

```python
from timm.data import create_transform

transform = create_transform(
    input_size=224,
    is_training=True,
    auto_augment='rand-m9-mstd0.5-inc1',  # AutoAugment
    re_prob=0.25,  # Random Erasing
    mixup_alpha=0.8,  # Mixup
    cutmix_alpha=1.0  # CutMix
)
```

### 4. ä¼˜åŒ–æŠ€å·§

```python
# 1. ä½¿ç”¨AdamWä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)

# 2. Cosineå­¦ä¹ ç‡
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

# 3. Warmup
warmup_epochs = 20
for epoch in range(warmup_epochs):
    lr = base_lr * (epoch + 1) / warmup_epochs
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

# 4. Layer-wise LR Decay
for layer_id, (name, param) in enumerate(model.named_parameters()):
    lr_scale = 0.95 ** (num_layers - layer_id)
    param_group = {'params': param, 'lr': base_lr * lr_scale}
```

## æ€»ç»“

### Vision Transformerçš„è´¡çŒ®

1. **è¯æ˜äº†Transformeråœ¨CVçš„å¯è¡Œæ€§**
2. **æ‰“ç ´äº†CNNçš„å„æ–­**
3. **å¯å‘äº†å¤§é‡åç»­ç ”ç©¶**
4. **æ¨åŠ¨äº†è§†è§‰-è¯­è¨€ç»Ÿä¸€å»ºæ¨¡**

### Swin Transformerçš„è´¡çŒ®

1. **çª—å£æ³¨æ„åŠ›æœºåˆ¶**ï¼šé™ä½å¤æ‚åº¦
2. **å±‚æ¬¡åŒ–è®¾è®¡**ï¼šé€‚åˆå¯†é›†é¢„æµ‹
3. **ç›¸å¯¹ä½ç½®åç½®**ï¼šæ›´å¥½çš„ä½ç½®å»ºæ¨¡
4. **SOTAæ€§èƒ½**ï¼šå¤šä¸ªä»»åŠ¡åˆ·æ–°è®°å½•

### å…³é”®å¯ç¤º

* **Transformeræ˜¯é€šç”¨æ¶æ„**ï¼šä¸åªæ˜¯NLP
* **å½’çº³åç½®çš„æƒè¡¡**ï¼šå°‘vså¤šï¼Œæ•°æ®vså…ˆéªŒ
* **å±‚æ¬¡åŒ–å¾ˆé‡è¦**ï¼šå¤šå°ºåº¦ç‰¹å¾ä¸å¯æˆ–ç¼º
* **å±€éƒ¨+å…¨å±€**ï¼šçª—å£æ³¨æ„åŠ›çš„æ™ºæ…§

## å½±å“ä¸å±•æœ›

### Transformeråœ¨CVçš„å½±å“

* ğŸ“Š åˆ·æ–°äº†å¤šä¸ªè§†è§‰ä»»åŠ¡çš„SOTA
* ğŸ”§ å‚¬ç”Ÿäº†å¤§é‡Transformerå˜ä½“
* ğŸš€ æ¨åŠ¨äº†è§†è§‰åŸºç¡€æ¨¡å‹çš„å‘å±•
* ğŸ“ ç»Ÿä¸€äº†è§†è§‰å’Œè¯­è¨€çš„æ¶æ„

### æœªæ¥æ–¹å‘

1. **æ•ˆç‡ä¼˜åŒ–**ï¼šé™ä½è®¡ç®—å¤æ‚åº¦
2. **å°æ•°æ®å­¦ä¹ **ï¼šå‡å°‘æ•°æ®ä¾èµ–
3. **å¤šæ¨¡æ€èåˆ**ï¼šè§†è§‰+è¯­è¨€+...
4. **å¯è§£é‡Šæ€§**ï¼šç†è§£Transformerå­¦åˆ°äº†ä»€ä¹ˆ

## å‚è€ƒèµ„æ–™

1. Dosovitskiy, A., et al. (2020). An Image is Worth 16x16 Words
2. Liu, Z., et al. (2021). Swin Transformer
3. [æˆ‘çš„GitHubä»£ç ä»“åº“](https://github.com/YangCazz/DeepLearning)
4. [ViTè®ºæ–‡è§£è¯»](https://arxiv.org/abs/2010.11929)
5. [Swin Transformerè®ºæ–‡è§£è¯»](https://arxiv.org/abs/2103.14030)

---

*è¿™æ˜¯æ·±åº¦å­¦ä¹ ç»å…¸ç½‘ç»œç³»åˆ—çš„æœ€åä¸€ç¯‡ã€‚ä»LeNetåˆ°Transformerï¼Œæˆ‘ä»¬è§è¯äº†æ·±åº¦å­¦ä¹ çš„è¾‰ç…Œå‘å±•ã€‚æ„Ÿè°¢å…³æ³¨ï¼*

