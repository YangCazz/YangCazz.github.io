---
layout: post
title: "Attentionæœºåˆ¶è¯¦è§£ï¼šæ·±åº¦å­¦ä¹ çš„æ³¨æ„åŠ›é©å‘½"
date: 2025-01-24
categories: [æ·±åº¦å­¦ä¹ , Attentionæœºåˆ¶]
tags: [Attention, Seq2Seq, Encoder-Decoder, NLP, æ·±åº¦å­¦ä¹ ]
excerpt: "æ·±å…¥è§£æAttentionæ³¨æ„åŠ›æœºåˆ¶çš„åŸç†ã€å‘å±•å†ç¨‹å’Œåº”ç”¨ã€‚ä»Seq2Seqåˆ°Self-Attentionï¼Œç†è§£å¦‚ä½•è®©æ¨¡å‹å…³æ³¨é‡è¦ä¿¡æ¯ã€‚"
---

# Attentionæœºåˆ¶è¯¦è§£ï¼šæ·±åº¦å­¦ä¹ çš„æ³¨æ„åŠ›é©å‘½

## å¼•è¨€

çºµè§‚æ·±åº¦å­¦ä¹ å‘å±•å†å²ï¼ŒGoogleçš„è´¡çŒ®æ˜¯æ— ä¸ä¼¦æ¯”çš„ã€‚Attentionæœºåˆ¶æœ€æ—©å‡ºç°åœ¨è§†è§‰é¢†åŸŸï¼Œä¹‹ååº”ç”¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå¹¶æˆä¸ºç°ä»£æ·±åº¦å­¦ä¹ æœ€é‡è¦çš„ç»„ä»¶ä¹‹ä¸€ã€‚

## åŸºæœ¬ä¿¡æ¯

### å¼€åˆ›æ€§è®ºæ–‡

* **[2014] è§†è§‰ä»»åŠ¡**ï¼š[Recurrent Models of Visual Attention](https://arxiv.org/abs/1406.6247)
* **[2014] æœºå™¨ç¿»è¯‘**ï¼š[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473v7)

## Attentionæœºåˆ¶çš„äº§ç”ŸèƒŒæ™¯

### RNNçš„å±€é™æ€§

åœ¨NLPä»»åŠ¡ä¸­ï¼Œéœ€è¦å¤„ç†**åºåˆ—æ•°æ®ï¼ˆSequenceï¼‰**ï¼š

![CNNå’ŒRNNçš„å±€é™](/assets/images/deep-learning/Attention_CNN_RNN.png)

#### RNNçš„é—®é¢˜

1. **é•¿æœŸä¾èµ–é—®é¢˜**ï¼šéš¾ä»¥æ•æ‰è¿œè·ç¦»çš„ä¾èµ–å…³ç³»
2. **ä¸²è¡Œè®¡ç®—**ï¼šæ— æ³•å¹¶è¡Œï¼Œè®­ç»ƒæ…¢
3. **ä¿¡æ¯ç“¶é¢ˆ**ï¼šå›ºå®šé•¿åº¦çš„éšçŠ¶æ€éš¾ä»¥ç¼–ç æ‰€æœ‰ä¿¡æ¯
4. **ç­‰æƒé‡å¤„ç†**ï¼šæ‰€æœ‰è¾“å…¥è¢«å¹³ç­‰å¯¹å¾…

**å…³é”®é—®é¢˜**ï¼šåœ¨åºåˆ—ä¸­ï¼Œä¸åŒä½ç½®çš„å…ƒç´ å¯¹å½“å‰é¢„æµ‹çš„é‡è¦æ€§æ˜¯**ä¸åŒçš„**ï¼

### Encoder-Decoderæ¡†æ¶

![Encoder-Decoder](/assets/images/deep-learning/Attention_Encoder_Decoder.png)

**æ ‡å‡†Encoder-Decoder**ï¼ˆ2014å¹´æå‡ºï¼‰ï¼š

$$
\text{Encoder}: \mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1})
$$

$$
\text{Context}: \mathbf{c} = q(\mathbf{h}_1, ..., \mathbf{h}_T)
$$

$$
\text{Decoder}: \mathbf{y}_t = g(\mathbf{c}, \mathbf{y}_1, ..., \mathbf{y}_{t-1})
$$

**é—®é¢˜**ï¼šæ‰€æœ‰ \(\mathbf{y}_i\) éƒ½ä¾èµ–äºåŒä¸€ä¸ªå›ºå®šçš„ \(\mathbf{c}\)ï¼Œæƒé‡ç›¸åŒï¼

## Attentionæœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³

### åŠ¨æ€è¯­ä¹‰ç¼–ç 

Attentionçš„å…³é”®åˆ›æ–°ï¼š**è®©æ¯ä¸ªè¾“å‡ºå…³æ³¨ä¸åŒçš„è¾“å…¥éƒ¨åˆ†**ã€‚

$$
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
$$

å…¶ä¸­ \(\alpha_{ij}\) æ˜¯æ³¨æ„åŠ›æƒé‡ï¼Œè¡¨ç¤ºç”Ÿæˆ \(\mathbf{y}_i\) æ—¶å¯¹ \(\mathbf{h}_j\) çš„å…³æ³¨ç¨‹åº¦ã€‚

### æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—

![Attentionæƒé‡è®¡ç®—](/assets/images/deep-learning/Attention_index.png)

#### ç¬¬1æ­¥ï¼šè®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°

$$
e_{ij} = a(\mathbf{s}_{i-1}, \mathbf{h}_j)
$$

å¸¸ç”¨ç›¸ä¼¼åº¦å‡½æ•°ï¼š
* **ç‚¹ç§¯**ï¼š\(a(\mathbf{s}, \mathbf{h}) = \mathbf{s}^T \mathbf{h}\)
* **ç¼©æ”¾ç‚¹ç§¯**ï¼š\(a(\mathbf{s}, \mathbf{h}) = \frac{\mathbf{s}^T \mathbf{h}}{\sqrt{d}}\)
* **åŠ æ€§**ï¼š\(a(\mathbf{s}, \mathbf{h}) = \mathbf{v}^T \tanh(\mathbf{W}_1\mathbf{s} + \mathbf{W}_2\mathbf{h})\)
* **åŒçº¿æ€§**ï¼š\(a(\mathbf{s}, \mathbf{h}) = \mathbf{s}^T \mathbf{W} \mathbf{h}\)

#### ç¬¬2æ­¥ï¼šå½’ä¸€åŒ–ï¼ˆSoftmaxï¼‰

$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
$$

#### ç¬¬3æ­¥ï¼šåŠ æƒæ±‚å’Œ

$$
\mathbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j
$$

### Attentionçš„æœ¬è´¨

![Attentionè·¯å¾„](/assets/images/deep-learning/Attention_path_real.png)

**è½¯å¯»å€ï¼ˆSoft Addressingï¼‰**ï¼š

å°†Sourceçœ‹ä½œå­˜å‚¨å™¨ï¼š
* **Key**ï¼šåœ°å€
* **Value**ï¼šå†…å®¹

å¯¹äºQueryï¼š
1. è®¡ç®—Queryä¸æ¯ä¸ªKeyçš„ç›¸ä¼¼åº¦
2. å½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡
3. å¯¹ValueåŠ æƒæ±‚å’Œ

$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
$$

## Attentionçš„æ•°å­¦è¡¨è¾¾

### é€šç”¨å…¬å¼

$$
\text{Attention}(Q, K, V) = \sum_{i} \text{Similarity}(Q, K_i) \cdot V_i
$$

### PyTorchå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicAttention(nn.Module):
    """åŸºç¡€çš„Attentionæœºåˆ¶"""
    def __init__(self, hidden_dim):
        super(BasicAttention, self).__init__()
        self.hidden_dim = hidden_dim
        
        # æ³¨æ„åŠ›æƒé‡è®¡ç®—
        self.W_q = nn.Linear(hidden_dim, hidden_dim)
        self.W_k = nn.Linear(hidden_dim, hidden_dim)
        self.W_v = nn.Linear(hidden_dim, hidden_dim)
    
    def forward(self, query, key, value, mask=None):
        """
        query: (batch, query_len, hidden_dim)
        key: (batch, key_len, hidden_dim)
        value: (batch, value_len, hidden_dim)
        """
        # 1. çº¿æ€§å˜æ¢
        Q = self.W_q(query)  # (batch, query_len, hidden_dim)
        K = self.W_k(key)    # (batch, key_len, hidden_dim)
        V = self.W_v(value)  # (batch, value_len, hidden_dim)
        
        # 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼ˆç¼©æ”¾ç‚¹ç§¯ï¼‰
        scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, query_len, key_len)
        scores = scores / torch.sqrt(torch.tensor(self.hidden_dim, dtype=torch.float32))
        
        # 3. åº”ç”¨maskï¼ˆå¯é€‰ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 4. Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(scores, dim=-1)  # (batch, query_len, key_len)
        
        # 5. åŠ æƒæ±‚å’Œ
        context = torch.matmul(attention_weights, V)  # (batch, query_len, hidden_dim)
        
        return context, attention_weights
```

## Attentionçš„å˜ä½“

### 1. Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰

**ç‰¹ç‚¹**ï¼šQueryã€Keyã€Valueæ¥è‡ªåŒä¸€ä¸ªåºåˆ—ã€‚

```python
class SelfAttention(nn.Module):
    def __init__(self, hidden_dim):
        super(SelfAttention, self).__init__()
        self.attention = BasicAttention(hidden_dim)
    
    def forward(self, x, mask=None):
        # x: (batch, seq_len, hidden_dim)
        # Self-Attention: Q=K=V=x
        context, attention_weights = self.attention(x, x, x, mask)
        return context, attention_weights
```

**åº”ç”¨**ï¼šTransformerã€BERTã€GPTç­‰ã€‚

### 2. Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰

**æ€æƒ³**ï¼šä½¿ç”¨å¤šç»„ä¸åŒçš„Qã€Kã€VçŸ©é˜µï¼Œæ•è·ä¸åŒçš„å…³ç³»ã€‚

```python
class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, hidden_dim, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert hidden_dim % num_heads == 0
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        self.W_q = nn.Linear(hidden_dim, hidden_dim)
        self.W_k = nn.Linear(hidden_dim, hidden_dim)
        self.W_v = nn.Linear(hidden_dim, hidden_dim)
        self.W_o = nn.Linear(hidden_dim, hidden_dim)
    
    def split_heads(self, x):
        """åˆ†å‰²æˆå¤šä¸ªå¤´"""
        batch_size, seq_len, hidden_dim = x.size()
        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)
        return x.transpose(1, 2)  # (batch, num_heads, seq_len, head_dim)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 1. çº¿æ€§å˜æ¢
        Q = self.W_q(query)
        K = self.W_k(key)
        V = self.W_v(value)
        
        # 2. åˆ†å‰²æˆå¤šå¤´
        Q = self.split_heads(Q)  # (batch, num_heads, query_len, head_dim)
        K = self.split_heads(K)
        V = self.split_heads(V)
        
        # 3. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, num_heads, query_len, key_len)
        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        context = torch.matmul(attention_weights, V)  # (batch, num_heads, query_len, head_dim)
        
        # 4. åˆå¹¶å¤šå¤´
        context = context.transpose(1, 2).contiguous()  # (batch, query_len, num_heads, head_dim)
        context = context.view(batch_size, -1, self.hidden_dim)  # (batch, query_len, hidden_dim)
        
        # 5. è¾“å‡ºæŠ•å½±
        output = self.W_o(context)
        
        return output, attention_weights
```

### 3. Cross-Attentionï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰

**ç‰¹ç‚¹**ï¼šQueryæ¥è‡ªä¸€ä¸ªåºåˆ—ï¼ŒKeyå’ŒValueæ¥è‡ªå¦ä¸€ä¸ªåºåˆ—ã€‚

**åº”ç”¨**ï¼šæœºå™¨ç¿»è¯‘ã€å›¾åƒå­—å¹•ç”Ÿæˆç­‰ã€‚

## Attentionåœ¨NLPä¸­çš„åº”ç”¨

### æœºå™¨ç¿»è¯‘

![RNN Attention](/assets/images/deep-learning/Attention_RNN_index.png)

åœ¨ç¿»è¯‘"The cat sat on the mat"åˆ°"çŒ«ååœ¨å«å­ä¸Š"æ—¶ï¼š
* ç¿»è¯‘"çŒ«"æ—¶ï¼Œä¸»è¦å…³æ³¨"cat"
* ç¿»è¯‘"å"æ—¶ï¼Œä¸»è¦å…³æ³¨"sat"
* ç¿»è¯‘"å«å­"æ—¶ï¼Œä¸»è¦å…³æ³¨"mat"

### æ–‡æœ¬æ‘˜è¦

Attentionå¸®åŠ©æ¨¡å‹ï¼š
* è¯†åˆ«æ–‡ç« çš„å…³é”®å¥å­
* è¿‡æ»¤å†—ä½™ä¿¡æ¯
* ç”Ÿæˆç®€æ´çš„æ‘˜è¦

### é—®ç­”ç³»ç»Ÿ

Attentionå¸®åŠ©æ¨¡å‹ï¼š
* åœ¨æ–‡æ¡£ä¸­å®šä½ç­”æ¡ˆ
* ç†è§£é—®é¢˜ä¸æ®µè½çš„å…³è”
* æŠ½å–æˆ–ç”Ÿæˆç­”æ¡ˆ

## Attention vs CNN vs RNN

| ç‰¹æ€§ | CNN | RNN | Attention |
|------|-----|-----|-----------|
| æ„Ÿå—é‡ | å±€éƒ¨ï¼ˆå¯å åŠ ä¸ºå…¨å±€ï¼‰ | å…¨å±€ï¼ˆé€’å½’ï¼‰ | å…¨å±€ï¼ˆç›´æ¥ï¼‰ |
| å¹¶è¡Œæ€§ | é«˜ | ä½ | é«˜ |
| é•¿ä¾èµ– | ä¸­ç­‰ | å·® | å¥½ |
| è®¡ç®—å¤æ‚åº¦ | O(n) | O(n) | O(nÂ²) |
| ä½ç½®ä¿¡æ¯ | éšå¼ | éšå¼ | éœ€è¦æ˜¾å¼ç¼–ç  |

## Attentionçš„ä¼˜åŠ¿

### 1. è§£å†³é•¿ä¾èµ–é—®é¢˜

**RNNçš„é—®é¢˜**ï¼šä¿¡æ¯ç»è¿‡å¤šæ­¥ä¼ é€’ä¼šè¡°å‡ã€‚

**Attentionçš„è§£å†³**ï¼šç›´æ¥å»ºç«‹ä»»æ„ä¸¤ä¸ªä½ç½®çš„è¿æ¥ã€‚

### 2. æé«˜å¹¶è¡Œæ€§

**RNNçš„é—®é¢˜**ï¼šå¿…é¡»é¡ºåºè®¡ç®—ã€‚

**Attentionçš„è§£å†³**ï¼šæ‰€æœ‰ä½ç½®å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚

### 3. å¯è§£é‡Šæ€§

**å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡**ï¼š

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, source_tokens, target_tokens):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    plt.figure(figsize=(10, 8))
    sns.heatmap(attention_weights, 
                xticklabels=source_tokens, 
                yticklabels=target_tokens,
                cmap='viridis',
                annot=True,
                fmt='.2f')
    plt.xlabel('Source')
    plt.ylabel('Target')
    plt.title('Attention Weights')
    plt.show()
```

### 4. æ€§èƒ½æå‡

åœ¨å„ç±»NLPä»»åŠ¡ä¸Šï¼ŒAttentionéƒ½å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## Attentionçš„ç¼ºç‚¹

### 1. è®¡ç®—å¤æ‚åº¦é«˜

å¯¹äºé•¿åº¦ä¸ºnçš„åºåˆ—ï¼š
* **ç©ºé—´å¤æ‚åº¦**ï¼šO(nÂ²)
* **æ—¶é—´å¤æ‚åº¦**ï¼šO(nÂ²Â·d)

### 2. ç¼ºå°‘ä½ç½®ä¿¡æ¯

Attentionæœ¬èº«ä¸è€ƒè™‘é¡ºåºï¼Œéœ€è¦é¢å¤–çš„ä½ç½®ç¼–ç ã€‚

### 3. è®­ç»ƒæ•°æ®éœ€æ±‚

éœ€è¦æ›´å¤šæ•°æ®æ‰èƒ½å……åˆ†è®­ç»ƒã€‚

## ä½ç½®ç¼–ç ï¼ˆPosition Encodingï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ

Attentionæ˜¯**ä½ç½®æ— å…³çš„**ï¼Œéœ€è¦æ˜¾å¼å‘Šè¯‰æ¨¡å‹ä½ç½®ä¿¡æ¯ã€‚

### ç»å¯¹ä½ç½®ç¼–ç 

**Sinusoidal Position Encoding**ï¼ˆTransformerä½¿ç”¨ï¼‰ï¼š

$$
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$

$$
PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                            (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x: (batch, seq_len, d_model)
        x = x + self.pe[:, :x.size(1), :]
        return x
```

### å¯å­¦ä¹ ä½ç½®ç¼–ç 

```python
class LearnedPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(LearnedPositionalEncoding, self).__init__()
        self.pos_embedding = nn.Embedding(max_len, d_model)
    
    def forward(self, x):
        # x: (batch, seq_len, d_model)
        batch_size, seq_len, _ = x.size()
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).repeat(batch_size, 1)
        x = x + self.pos_embedding(positions)
        return x
```

## Attentionçš„æ¼”è¿›

```
2014: åŸºç¡€Attentionï¼ˆæœºå™¨ç¿»è¯‘ï¼‰
  â†“
2015: å„ç±»å˜ä½“ï¼ˆå›¾åƒå­—å¹•ã€æ–‡æœ¬æ‘˜è¦ï¼‰
  â†“
2017: Self-Attention / Transformer
  â†“
2018: BERTï¼ˆåŒå‘Self-Attentionï¼‰
  â†“
2019: GPT-2ï¼ˆå•å‘Self-Attentionï¼‰
  â†“
2020: Vision Transformerï¼ˆAttentionç”¨äºCVï¼‰
  â†“
2021: Swin Transformerï¼ˆçª—å£Attentionï¼‰
```

## æ€»ç»“

### Attentionçš„æ ¸å¿ƒæ€æƒ³

1. **é€‰æ‹©æ€§å…³æ³¨**ï¼šä¸åŒè¾“å…¥æœ‰ä¸åŒçš„é‡è¦æ€§
2. **åŠ¨æ€æƒé‡**ï¼šæ ¹æ®QueryåŠ¨æ€è®¡ç®—æƒé‡
3. **è½¯å¯»å€**ï¼šå¯å¾®åˆ†çš„ä¿¡æ¯æ£€ç´¢æœºåˆ¶

### å…³é”®ç»„ä»¶

* **Queryï¼ˆæŸ¥è¯¢ï¼‰**ï¼šæˆ‘è¦ä»€ä¹ˆï¼Ÿ
* **Keyï¼ˆé”®ï¼‰**ï¼šæˆ‘æ˜¯ä»€ä¹ˆï¼Ÿ
* **Valueï¼ˆå€¼ï¼‰**ï¼šæˆ‘æœ‰ä»€ä¹ˆï¼Ÿ

### è®¡ç®—æµç¨‹

```
Query + Key â†’ Similarity â†’ Softmax â†’ Weights
Weights + Value â†’ Weighted Sum â†’ Output
```

### Attentionçš„å½±å“

Attentionæœºåˆ¶ï¼š
* ğŸ“Š æˆä¸ºç°ä»£NLPçš„æ ¸å¿ƒç»„ä»¶
* ğŸ”§ å‚¬ç”Ÿäº†Transformeré©å‘½
* ğŸš€ æ¨åŠ¨äº†å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•
* ğŸ“ å¯å‘äº†ä¼—å¤šåç»­ç ”ç©¶

**Attentionæ”¹å˜äº†æ·±åº¦å­¦ä¹ çš„èŒƒå¼ï¼**

## å®è·µå»ºè®®

### 1. ä½•æ—¶ä½¿ç”¨Attentionï¼Ÿ

âœ… **é€‚ç”¨åœºæ™¯**ï¼š
* åºåˆ—åˆ°åºåˆ—ä»»åŠ¡
* é•¿è·ç¦»ä¾èµ–
* éœ€è¦å¯è§£é‡Šæ€§
* å˜é•¿è¾“å…¥

âŒ **ä¸é€‚ç”¨**ï¼š
* æé•¿åºåˆ—ï¼ˆè€ƒè™‘ç¨€ç–Attentionï¼‰
* è®¡ç®—èµ„æºå—é™
* ä½ç½®ä¿¡æ¯æå…¶é‡è¦

### 2. Attentionçš„è°ƒä¼˜

```python
# 1. è°ƒæ•´æ³¨æ„åŠ›å¤´æ•°
num_heads = 8  # å¸¸ç”¨å€¼ï¼š4, 8, 12, 16

# 2. ä½¿ç”¨Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
attention = nn.Dropout(p=0.1)(attention)

# 3. æ·»åŠ Layer Normalization
output = nn.LayerNorm(hidden_dim)(output)

# 4. ä½¿ç”¨warmupå­¦ä¹ ç‡ç­–ç•¥
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)
```

## å‚è€ƒèµ„æ–™

1. Bahdanau, D., et al. (2014). Neural Machine Translation by Jointly Learning to Align and Translate
2. Mnih, V., et al. (2014). Recurrent Models of Visual Attention
3. Vaswani, A., et al. (2017). Attention Is All You Need
4. [Attentionè®ºæ–‡è§£è¯»](https://arxiv.org/abs/1409.0473)

---

*è¿™æ˜¯æ·±åº¦å­¦ä¹ ç»å…¸ç½‘ç»œç³»åˆ—çš„ç¬¬å…«ç¯‡ï¼Œä¸‹ä¸€ç¯‡å°†ä»‹ç»Transformerä¸BERTã€‚æ¬¢è¿å…³æ³¨ï¼*

