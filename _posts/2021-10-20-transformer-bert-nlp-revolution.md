---
layout: post
title: "Transformerä¸BERTï¼šè‡ªç„¶è¯­è¨€å¤„ç†çš„é©å‘½"
date: 2021-10-20 10:00:00 +0800
categories: [æ·±åº¦å­¦ä¹ , Transformer]
tags: [Transformer, NLP, é¢„è®­ç»ƒæ¨¡å‹]
excerpt: "æ·±å…¥è§£æTransformerå’ŒBERTçš„åŸç†ä¸åˆ›æ–°ã€‚ä»'Attention Is All You Need'åˆ°é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæ¢ç´¢NLPé¢†åŸŸçš„èŒƒå¼è½¬å˜ã€‚"
---

# Transformerä¸BERTï¼šè‡ªç„¶è¯­è¨€å¤„ç†çš„é©å‘½

## å¼•è¨€

2017å¹´ï¼ŒGoogleå‘è¡¨äº†è®ºæ–‡ã€ŠAttention Is All You Needã€‹ï¼Œæå‡ºäº†Transformeræ¶æ„ã€‚è¿™ä¸€å·¥ä½œä¸ä»…é©æ–°äº†NLPé¢†åŸŸï¼Œæ›´å½±å“äº†æ•´ä¸ªæ·±åº¦å­¦ä¹ çš„å‘å±•æ–¹å‘ã€‚

**"Attention Is All You Need"** â€”â€” ä½ åªéœ€è¦æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸éœ€è¦RNNå’ŒCNNï¼

## 1. Transformer (2017)

### åŸºæœ¬ä¿¡æ¯

* **å›¢é˜Ÿ**ï¼šGoogle Brain
* **è®ºæ–‡**ï¼š[Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* **DOI**ï¼šarXiv:1706.03762
* **æ—¶é—´**ï¼š2017å¹´
* **å½±å“**ï¼šNLPé¢†åŸŸçš„é‡Œç¨‹ç¢‘

### ä¸ºä»€ä¹ˆéœ€è¦Transformerï¼Ÿ

#### RNN/CNNçš„å±€é™

**RNNçš„é—®é¢˜**ï¼š
* âŒ æ— æ³•å¹¶è¡Œè®¡ç®—
* âŒ é•¿ä¾èµ–é—®é¢˜
* âŒ æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

**CNNçš„é—®é¢˜**ï¼š
* âŒ æ„Ÿå—é‡å—é™
* âŒ éš¾ä»¥å»ºæ¨¡é•¿è·ç¦»ä¾èµ–

**è§£å†³æ–¹æ¡ˆ**ï¼š**å®Œå…¨åŸºäºAttentionæœºåˆ¶ï¼**

### Transformeræ¶æ„

![Transformerç»“æ„](/assets/images/deep-learning/Transformer.png)

![Transformerè·¯å¾„](/assets/images/deep-learning/Transformer_path.png)

#### æ•´ä½“ç»“æ„

**Encoder-Decoderæ¶æ„**ï¼š
* **Encoder**ï¼š6å±‚ï¼ˆè®ºæ–‡ä¸­ï¼‰
  - Multi-Head Self-Attention
  - Position-wise Feed-Forward Network
  
* **Decoder**ï¼š6å±‚
  - Masked Multi-Head Self-Attention
  - Encoder-Decoder Attention
  - Position-wise Feed-Forward Network

### æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### 1. Multi-Head Self-Attention

![Self-Attention](/assets/images/deep-learning/Self_Attention.png)

![Self-Attentionè®¡ç®—](/assets/images/deep-learning/Self_Attention_Caculation.png)

**å•å¤´æ³¨æ„åŠ›**ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**å¤šå¤´æ³¨æ„åŠ›**ï¼š

![Multi-Head Attention](/assets/images/deep-learning/Self_Attention_MultiHead.png)

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(head_1, ..., head_h)W^O
$$

$$
head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Q, K, Vçº¿æ€§å˜æ¢
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        
        # è¾“å‡ºçº¿æ€§å˜æ¢
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
        # Q, K, V: (batch, num_heads, seq_len, d_k)
        
        # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        # scores: (batch, num_heads, seq_len, seq_len)
        
        # 2. åº”ç”¨maskï¼ˆå¯é€‰ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 3. Softmax
        attention = F.softmax(scores, dim=-1)
        attention = self.dropout(attention)
        
        # 4. åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention, V)
        # output: (batch, num_heads, seq_len, d_k)
        
        return output, attention
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 1. çº¿æ€§å˜æ¢
        Q = self.W_q(query)  # (batch, seq_len, d_model)
        K = self.W_k(key)
        V = self.W_v(value)
        
        # 2. åˆ†å‰²æˆå¤šå¤´
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        # Q, K, V: (batch, num_heads, seq_len, d_k)
        
        # 3. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        x, attention = self.scaled_dot_product_attention(Q, K, V, mask)
        # x: (batch, num_heads, seq_len, d_k)
        
        # 4. åˆå¹¶å¤šå¤´
        x = x.transpose(1, 2).contiguous()
        # x: (batch, seq_len, num_heads, d_k)
        
        x = x.view(batch_size, -1, self.d_model)
        # x: (batch, seq_len, d_model)
        
        # 5. è¾“å‡ºæŠ•å½±
        x = self.W_o(x)
        
        return x, attention
```

#### 2. Position-wise Feed-Forward Network

**å‰é¦ˆç½‘ç»œ**ï¼šä¸¤å±‚å…¨è¿æ¥ï¼Œç¬¬ä¸€å±‚å¸¦ReLUæ¿€æ´»ã€‚

$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

```python
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        # x: (batch, seq_len, d_model)
        x = self.fc1(x)      # (batch, seq_len, d_ff)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)      # (batch, seq_len, d_model)
        return x
```

#### 3. Positional Encoding

**ä½ç½®ç¼–ç **ï¼šç”±äºSelf-Attentionæœ¬èº«ä¸è€ƒè™‘ä½ç½®ï¼Œéœ€è¦æ·»åŠ ä½ç½®ä¿¡æ¯ã€‚

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                            (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x: (batch, seq_len, d_model)
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)
```

#### 4. Layer Normalization

**å±‚å½’ä¸€åŒ–**ï¼šåœ¨ç‰¹å¾ç»´åº¦ä¸Šå½’ä¸€åŒ–ã€‚

$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

```python
class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(features))
        self.beta = nn.Parameter(torch.zeros(features))
        self.eps = eps
    
    def forward(self, x):
        # x: (batch, seq_len, features)
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
```

#### 5. æ®‹å·®è¿æ¥

**Add & Norm**ï¼šæ¯ä¸ªå­å±‚éƒ½æœ‰æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ã€‚

$$
\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))
$$

### Encoder Layer

```python
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(EncoderLayer, self).__init__()
        
        # Multi-Head Self-Attention
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        
        # Feed-Forward Network
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # x: (batch, seq_len, d_model)
        
        # 1. Multi-Head Self-Attention + Add & Norm
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 2. Feed-Forward Network + Add & Norm
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))
        
        return x
```

### Decoder Layer

```python
class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(DecoderLayer, self).__init__()
        
        # Masked Multi-Head Self-Attention
        self.masked_self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        
        # Encoder-Decoder Attention
        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)
        
        # Feed-Forward Network
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):
        # x: (batch, tgt_seq_len, d_model)
        # enc_output: (batch, src_seq_len, d_model)
        
        # 1. Masked Multi-Head Self-Attention
        attn_output, _ = self.masked_self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 2. Encoder-Decoder Attention
        attn_output, _ = self.enc_dec_attn(x, enc_output, enc_output, src_mask)
        x = self.norm2(x + self.dropout(attn_output))
        
        # 3. Feed-Forward Network
        ffn_output = self.ffn(x)
        x = self.norm3(x + self.dropout(ffn_output))
        
        return x
```

### å®Œæ•´Transformer

```python
class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, 
                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, 
                 max_seq_len=5000, dropout=0.1):
        super(Transformer, self).__init__()
        
        # Embeddings
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # Positional Encoding
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)
        
        # Encoder
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout) 
            for _ in range(num_encoder_layers)
        ])
        
        # Decoder
        self.decoder_layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_decoder_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model
    
    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        # src: (batch, src_seq_len)
        # tgt: (batch, tgt_seq_len)
        
        # 1. Embedding + Positional Encoding
        src = self.src_embedding(src) * math.sqrt(self.d_model)
        src = self.pos_encoding(src)
        
        tgt = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        tgt = self.pos_encoding(tgt)
        
        # 2. Encoder
        for layer in self.encoder_layers:
            src = layer(src, src_mask)
        
        # 3. Decoder
        for layer in self.decoder_layers:
            tgt = layer(tgt, src, src_mask, tgt_mask)
        
        # 4. è¾“å‡º
        output = self.fc_out(tgt)
        
        return output
```

### Transformerçš„ä¼˜åŠ¿

1. **å¹¶è¡ŒåŒ–**ï¼šå®Œå…¨å¹¶è¡Œï¼Œè®­ç»ƒå¿«
2. **é•¿ä¾èµ–**ï¼šç›´æ¥å»ºæ¨¡ä»»æ„è·ç¦»çš„ä¾èµ–
3. **å¯è§£é‡Šæ€§**ï¼šæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
4. **é€šç”¨æ€§**ï¼šé€‚ç”¨äºå„ç±»åºåˆ—ä»»åŠ¡

### Transformerçš„åº”ç”¨

* âœ… **æœºå™¨ç¿»è¯‘**ï¼šGoogle Translate
* âœ… **æ–‡æœ¬ç”Ÿæˆ**ï¼šGPTç³»åˆ—
* âœ… **è¯­è¨€ç†è§£**ï¼šBERTç³»åˆ—
* âœ… **å¯¹è¯ç³»ç»Ÿ**ï¼šChatGPT
* âœ… **ä»£ç ç”Ÿæˆ**ï¼šCodex, GitHub Copilot
* âœ… **è®¡ç®—æœºè§†è§‰**ï¼šVision Transformer

## 2. BERT (2018)

### åŸºæœ¬ä¿¡æ¯

* **å›¢é˜Ÿ**ï¼šGoogle AI Language
* **è®ºæ–‡**ï¼š[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
* **DOI**ï¼šarXiv:1810.04805
* **æ—¶é—´**ï¼š2018å¹´

### BERTçš„åˆ›æ–°

#### 1. åŒå‘Transformer

![BERTç»“æ„](/assets/images/deep-learning/BERT.png)

**ä¸GPTçš„åŒºåˆ«**ï¼š
* **GPT**ï¼šå•å‘ï¼ˆä»å·¦åˆ°å³ï¼‰
* **BERT**ï¼šåŒå‘ï¼ˆåŒæ—¶çœ‹å·¦å³ï¼‰

**ä¼˜åŠ¿**ï¼šæ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡ã€‚

#### 2. é¢„è®­ç»ƒ + å¾®è°ƒèŒƒå¼

**ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼š
1. **Pre-training**ï¼šåœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®ä¸Šé¢„è®­ç»ƒ
2. **Fine-tuning**ï¼šåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒ

#### 3. ä¸‰ç§åµŒå…¥

**Token Embedding + Segment Embedding + Position Embedding**

```python
# BERTçš„è¾“å…¥è¡¨ç¤º
input_embedding = token_embedding + segment_embedding + position_embedding
```

### BERTçš„é¢„è®­ç»ƒä»»åŠ¡

#### ä»»åŠ¡1ï¼šMasked Language Model (MLM)

**æ€æƒ³**ï¼šéšæœºé®è”½15%çš„è¯ï¼Œè®©æ¨¡å‹é¢„æµ‹ã€‚

```
è¾“å…¥ï¼šThe [MASK] sat on the [MASK].
ç›®æ ‡ï¼šé¢„æµ‹ cat å’Œ mat
```

**å®ç°**ï¼š
* 80%çš„æ—¶é—´ï¼šç”¨[MASK]æ›¿æ¢
* 10%çš„æ—¶é—´ï¼šç”¨éšæœºè¯æ›¿æ¢
* 10%çš„æ—¶é—´ï¼šä¿æŒä¸å˜

#### ä»»åŠ¡2ï¼šNext Sentence Prediction (NSP)

**æ€æƒ³**ï¼šåˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸é‚»ã€‚

```
è¾“å…¥Aï¼š[CLS] The cat sat. [SEP] It was tired. [SEP]
æ ‡ç­¾ï¼šIsNext

è¾“å…¥Bï¼š[CLS] The cat sat. [SEP] The dog ran. [SEP]
æ ‡ç­¾ï¼šNotNext
```

### BERTçš„æ¶æ„

**ä¸¤ç§è§„æ¨¡**ï¼š

| æ¨¡å‹ | å±‚æ•° | éšè—å±‚å¤§å° | æ³¨æ„åŠ›å¤´æ•° | å‚æ•°é‡ |
|------|------|-----------|----------|--------|
| BERT-Base | 12 | 768 | 12 | 110M |
| BERT-Large | 24 | 1024 | 16 | 340M |

### BERTçš„ä½¿ç”¨

#### 1. å¥å­åˆ†ç±»

```python
# ä½¿ç”¨[CLS]çš„è¾“å‡º
class BERTClassifier(nn.Module):
    def __init__(self, bert_model, num_classes):
        super(BERTClassifier, self).__init__()
        self.bert = bert_model
        self.classifier = nn.Linear(bert_model.config.hidden_size, num_classes)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token
        logits = self.classifier(cls_output)
        return logits
```

#### 2. Tokenåˆ†ç±»ï¼ˆNERï¼‰

```python
# ä½¿ç”¨æ¯ä¸ªtokençš„è¾“å‡º
class BERTTokenClassifier(nn.Module):
    def __init__(self, bert_model, num_labels):
        super(BERTTokenClassifier, self).__init__()
        self.bert = bert_model
        self.classifier = nn.Linear(bert_model.config.hidden_size, num_labels)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state  # All tokens
        logits = self.classifier(sequence_output)
        return logits
```

#### 3. é—®ç­”ç³»ç»Ÿ

```python
# é¢„æµ‹ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®
class BERTForQuestionAnswering(nn.Module):
    def __init__(self, bert_model):
        super(BERTForQuestionAnswering, self).__init__()
        self.bert = bert_model
        self.qa_outputs = nn.Linear(bert_model.config.hidden_size, 2)  # start & end
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        
        logits = self.qa_outputs(sequence_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        
        return start_logits.squeeze(-1), end_logits.squeeze(-1)
```

### BERTçš„å½±å“

**BERTå¼€å¯äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ—¶ä»£**ï¼š

```
BERT (2018)
  â†“
RoBERTa (2019): æ”¹è¿›è®­ç»ƒç­–ç•¥
  â†“
ALBERT (2019): å‚æ•°å…±äº«
  â†“
ELECTRA (2020): æ›´é«˜æ•ˆçš„é¢„è®­ç»ƒ
  â†“
DeBERTa (2020): è§£è€¦æ³¨æ„åŠ›
```

### æ€§èƒ½æå‡

BERTåœ¨11ä¸ªNLPä»»åŠ¡ä¸Šåˆ·æ–°SOTAï¼š

| ä»»åŠ¡ | ä¹‹å‰SOTA | BERT-Base | BERT-Large |
|------|---------|-----------|-----------|
| GLUE | 68.9 | 78.5 | 80.4 |
| SQuAD 1.1 | 84.1 | 88.5 | 90.9 |
| SQuAD 2.0 | 66.3 | 73.7 | 80.0 |

## Transformer vs RNN vs CNN

| ç»´åº¦ | RNN | CNN | Transformer |
|------|-----|-----|-------------|
| è®¡ç®—å¤æ‚åº¦ | O(n) | O(1) | O(nÂ²) |
| åºåˆ—æ“ä½œæ•° | O(n) | O(log n) | O(1) |
| æœ€é•¿è·¯å¾„ | O(n) | O(log n) | O(1) |
| å¹¶è¡Œæ€§ | ä½ | é«˜ | é«˜ |
| é•¿ä¾èµ– | å·® | ä¸­ | å¥½ |

## å®è·µå»ºè®®

### 1. ä½¿ç”¨é¢„è®­ç»ƒBERT

```python
from transformers import BertModel, BertTokenizer

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# ç¼–ç æ–‡æœ¬
text = "Hello, how are you?"
encoded = tokenizer(text, return_tensors='pt')

# è·å–è¡¨ç¤º
with torch.no_grad():
    outputs = model(**encoded)
    last_hidden_state = outputs.last_hidden_state
```

### 2. å¾®è°ƒæŠ€å·§

```python
# 1. ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

# 2. ä½¿ç”¨warmup
from transformers import get_linear_schedule_with_warmup

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=500,
    num_training_steps=10000
)

# 3. æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### 3. èŠ‚çœå†…å­˜

```python
# 1. æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4

for i, batch in enumerate(train_loader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 2. æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    outputs = model(**inputs)
    loss = criterion(outputs, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

## æ€»ç»“

### Transformerçš„é©å‘½æ€§

1. **Attention Is All You Need**ï¼šæ‘’å¼ƒRNNå’ŒCNN
2. **å®Œå…¨å¹¶è¡Œ**ï¼šè®­ç»ƒé€Ÿåº¦å¤§å¹…æå‡
3. **é•¿ä¾èµ–å»ºæ¨¡**ï¼šä»»æ„è·ç¦»ç›´æ¥è¿æ¥
4. **é€šç”¨æ¶æ„**ï¼šé€‚ç”¨äºNLPå’ŒCV

### BERTçš„çªç ´

1. **åŒå‘å»ºæ¨¡**ï¼šæ›´å¥½çš„ä¸Šä¸‹æ–‡ç†è§£
2. **é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼**ï¼šè¿ç§»å­¦ä¹ æ–°é«˜åº¦
3. **åˆ·æ–°SOTA**ï¼š11ä¸ªä»»åŠ¡å…¨é¢é¢†å…ˆ
4. **å‚¬ç”Ÿç”Ÿæ€**ï¼šæ— æ•°å˜ä½“å’Œåº”ç”¨

### å…³é”®å¯ç¤º

* **é¢„è®­ç»ƒå¾ˆé‡è¦**ï¼šå¤§è§„æ¨¡æ— ç›‘ç£å­¦ä¹ 
* **åŒå‘æ›´å¼º**ï¼šåŒæ—¶çœ‹å·¦å³ä¸Šä¸‹æ–‡
* **è§„æ¨¡æ•ˆåº”**ï¼šæ›´å¤§çš„æ¨¡å‹ï¼Œæ›´å¥½çš„æ•ˆæœ
* **è¿ç§»å­¦ä¹ **ï¼šä¸€æ¬¡é¢„è®­ç»ƒï¼Œå¤„å¤„ä½¿ç”¨

## å½±å“ä¸åº”ç”¨

Transformerå’ŒBERTï¼š
* ğŸ“Š é©æ–°äº†NLPé¢†åŸŸçš„æ–¹æ³•è®º
* ğŸ”§ å‚¬ç”Ÿäº†GPTã€ChatGPTç­‰å¤§è¯­è¨€æ¨¡å‹
* ğŸš€ æ‰©å±•åˆ°è®¡ç®—æœºè§†è§‰ï¼ˆViTï¼‰
* ğŸ“ æˆä¸ºæ·±åº¦å­¦ä¹ çš„ä¸»æµæ¶æ„

**Transformeræ”¹å˜äº†AIçš„æœªæ¥ï¼**

## å‚è€ƒèµ„æ–™

1. Vaswani, A., et al. (2017). Attention Is All You Need
2. Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers
3. [Transformerè®ºæ–‡è§£è¯»](https://arxiv.org/abs/1706.03762)
4. [BERTè®ºæ–‡è§£è¯»](https://arxiv.org/abs/1810.04805)
5. [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

---

*è¿™æ˜¯æ·±åº¦å­¦ä¹ ç»å…¸ç½‘ç»œç³»åˆ—çš„ç¬¬ä¹ç¯‡ï¼Œä¸‹ä¸€ç¯‡å°†ä»‹ç»Vision Transformerã€‚æ¬¢è¿å…³æ³¨ï¼*

