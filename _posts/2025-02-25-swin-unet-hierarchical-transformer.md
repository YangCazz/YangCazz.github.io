---
layout: post
title: "Swin-UNetï¼šå±‚çº§åŒ–Transformerç ´è§£åŒ»å­¦åˆ†å‰²æ•ˆç‡å›°å±€"
date: 2025-02-25
categories: [åŒ»å­¦å½±åƒ, å›¾åƒåˆ†å‰²]
tags: [æ·±åº¦å­¦ä¹ , Swin-UNet, Swin Transformer, Window Attention, å±‚çº§æ¶æ„]
excerpt: "æ·±å…¥å‰–æSwin-UNetå¦‚ä½•é€šè¿‡shifted windowså’Œå±‚çº§è®¾è®¡ï¼Œåœ¨ä¿æŒå…¨å±€å»ºæ¨¡èƒ½åŠ›çš„åŒæ—¶å¤§å¹…é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œæˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„å®ç”¨Transformeræ–¹æ¡ˆã€‚"
author: YangCazz
math: true
---

## ğŸ“‹ å¼•è¨€

åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](/2025/02/20/transunet-hybrid-architecture/)ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†TransUNetå¦‚ä½•å°†Transformerå¼•å…¥åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œé€šè¿‡å…¨å±€è‡ªæ³¨æ„åŠ›å»ºæ¨¡é•¿è·ç¦»ä¾èµ–ã€‚ç„¶è€Œï¼ŒTransUNetå­˜åœ¨ä¸€ä¸ªè‡´å‘½ç¼ºé™·ï¼š

**è‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦** \( O(N^2) \)

```
é—®é¢˜ç¤ºä¾‹ï¼š
å›¾åƒåˆ†è¾¨ç‡ï¼š512Ã—512
ä¸‹é‡‡æ ·åˆ°ï¼šH/8 Ã— W/8 = 64Ã—64 = 4096 tokens

è‡ªæ³¨æ„åŠ›è®¡ç®—ï¼š
- QK^TçŸ©é˜µï¼š4096 Ã— 4096 â‰ˆ 16.8Mæ¬¡ä¹˜æ³•
- å†…å­˜éœ€æ±‚ï¼šB Ã— H Ã— N Ã— N Ã— 4 bytes
  â†’  Batch=2, Heads=12 â†’ çº¦3GBï¼ˆä»…æ³¨æ„åŠ›ï¼‰

ç»“æœï¼š
âœ— é«˜åˆ†è¾¨ç‡å›¾åƒéš¾ä»¥å¤„ç†
âœ— è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æ…¢
âœ— GPUå†…å­˜æ¶ˆè€—å·¨å¤§
```

**Swin-UNet**ï¼ˆ2021ï¼‰é€šè¿‡**Shifted Window Attention**è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼š
- âœ… **å±€éƒ¨çª—å£æ³¨æ„åŠ›**ï¼šå¤æ‚åº¦ä» \( O(N^2) \) é™è‡³ \( O(N) \)
- âœ… **å±‚çº§åŒ–æ¶æ„**ï¼šç±»ä¼¼CNNçš„å¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”
- âœ… **è·¨çª—å£äº¤äº’**ï¼šshifted windowså®ç°å…¨å±€å»ºæ¨¡

---

## ğŸ¯ Swin Transformeræ ¸å¿ƒæ€æƒ³

### 1. Window-based Self-Attention

**æ ‡å‡†è‡ªæ³¨æ„åŠ›**ï¼šæ¯ä¸ªtokenä¸æ‰€æœ‰tokenäº¤äº’ï¼ˆ\( O(N^2) \)ï¼‰

**Window Attention**ï¼šå°†ç‰¹å¾å›¾åˆ’åˆ†ä¸º \( M \times M \) çš„çª—å£ï¼Œä»…åœ¨çª—å£å†…è®¡ç®—æ³¨æ„åŠ›ã€‚

```
ç‰¹å¾å›¾ï¼šHÃ—W
çª—å£å¤§å°ï¼šMÃ—Mï¼ˆå¦‚7Ã—7ï¼‰
çª—å£æ•°é‡ï¼š(H/M) Ã— (W/M)

æ¯ä¸ªçª—å£å†…ï¼š
- Tokensæ•°é‡ï¼šM^2
- æ³¨æ„åŠ›å¤æ‚åº¦ï¼šO(M^2 Ã— M^2) = O(M^4)

æ€»å¤æ‚åº¦ï¼š
O((H/M Ã— W/M) Ã— M^4) = O(HW Ã— M^2) = O(N Ã— M^2)
                                   â†‘
                                å¸¸æ•°M
```

**å¤æ‚åº¦å¯¹æ¯”**ï¼š

| æ–¹æ³• | å¤æ‚åº¦ | 512Ã—512å›¾åƒï¼ˆM=7ï¼‰ |
|------|--------|-------------------|
| æ ‡å‡†æ³¨æ„åŠ› | \( O(N^2) \) | \( O(262144^2) \approx 6.9 \times 10^{10} \) |
| Windowæ³¨æ„åŠ› | \( O(N \times M^2) \) | \( O(262144 \times 49) \approx 1.3 \times 10^7 \) |
| **åŠ é€Ÿæ¯”** | - | **çº¦5000å€** |

### 2. Shifted Window Mechanism

**é—®é¢˜**ï¼šå•çº¯çš„Window Attentionå‰²è£‚äº†çª—å£ä¹‹é—´çš„ä¿¡æ¯æµã€‚

```
Layer Lï¼šå¸¸è§„çª—å£åˆ’åˆ†
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ A â”‚ B â”‚ C â”‚ D â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ E â”‚ F â”‚ G â”‚ H â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

çª—å£å†…äº¤äº’ï¼šAå†…éƒ¨ã€Bå†…éƒ¨...
çª—å£é—´éš”ç¦»ï¼šAå’ŒBæ— æ³•äº¤äº’
```

**è§£å†³æ–¹æ¡ˆ**ï¼šäº¤æ›¿ä½¿ç”¨å¸¸è§„çª—å£å’Œç§»ä½çª—å£ï¼ˆShifted Windowsï¼‰

```
Layer Lï¼ˆå¸¸è§„ï¼‰ï¼š
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ A â”‚ B â”‚ C â”‚ D â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ E â”‚ F â”‚ G â”‚ H â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Layer L+1ï¼ˆç§»ä½M/2ï¼‰ï¼š
  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
  â”‚a â”‚ b â”‚ c â”‚ d â”‚
  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
  â”‚e â”‚ f â”‚ g â”‚ h â”‚
  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

æ•ˆæœï¼š
- Layer Lï¼šAå†…éƒ¨äº¤äº’
- Layer L+1ï¼šAçš„ä¸€éƒ¨åˆ†ä¸Bçš„ä¸€éƒ¨åˆ†ï¼ˆè·¨çª—å£ï¼‰
- å †å å¤šå±‚ â†’ å…¨å±€æ„Ÿå—é‡
```

**æ•°å­¦è¡¨ç¤º**ï¼š

è®¾çª—å£å¤§å°ä¸º \( M \)ï¼Œç§»ä½é‡ä¸º \( \lfloor M/2 \rfloor \)ã€‚

**Layer \( l \)**ï¼ˆå¸¸è§„çª—å£ï¼‰ï¼š

$$
z^l = \text{W-MSA}(\text{LN}(z^{l-1})) + z^{l-1}
$$

**Layer \( l+1 \)**ï¼ˆç§»ä½çª—å£ï¼‰ï¼š

$$
z^{l+1} = \text{SW-MSA}(\text{LN}(z^{l})) + z^{l}
$$

å…¶ä¸­ï¼š
- W-MSAï¼šWindow Multi-Head Self-Attention
- SW-MSAï¼šShifted Window Multi-Head Self-Attention
- LNï¼šLayer Normalization

### 3. å±‚çº§åŒ–æ¶æ„

Swin Transformeré‡‡ç”¨ç±»ä¼¼CNNçš„**ç‰¹å¾é‡‘å­—å¡”**ï¼š

```
è¾“å…¥å›¾åƒï¼šHÃ—WÃ—3
            â†“
Stage 1ï¼šH/4Ã—W/4Ã—C    [Patch Partition + Linear Embedding]
            â†“
         Swin Transformer Block Ã— 2
            â†“
Stage 2ï¼šH/8Ã—W/8Ã—2C   [Patch Merging]
            â†“
         Swin Transformer Block Ã— 2
            â†“
Stage 3ï¼šH/16Ã—W/16Ã—4C  [Patch Merging]
            â†“
         Swin Transformer Block Ã— 6
            â†“
Stage 4ï¼šH/32Ã—W/32Ã—8C  [Patch Merging]
            â†“
         Swin Transformer Block Ã— 2
```

**Patch Merging**ï¼šç±»ä¼¼CNNçš„poolingï¼Œé™ä½åˆ†è¾¨ç‡ï¼Œå¢åŠ é€šé“æ•°ã€‚

$$
\begin{aligned}
\text{Input:} & \quad H \times W \times C \\
\text{Concatenate 2Ã—2é‚»åŸŸ:} & \quad \frac{H}{2} \times \frac{W}{2} \times 4C \\
\text{Linear Projection:} & \quad \frac{H}{2} \times \frac{W}{2} \times 2C
\end{aligned}
$$

---

## ğŸ—ï¸ Swin-UNetæ¶æ„

### æ•´ä½“è®¾è®¡

Swin-UNet = **Swin Transformerç¼–ç å™¨** + **Swin Transformerè§£ç å™¨** + **Skip Connections**

```
ç¼–ç å™¨                       è§£ç å™¨
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input (HÃ—WÃ—3)
    â†“
Patch Partition â†’ H/4Ã—W/4Ã—C  â”€â”
    â†“                          â”‚
Swin Ã— 2       â†’ H/4Ã—W/4Ã—C  â”€â”¼â”€â†’ Skip â”€â†’ PatchExpand â†’ H/4Ã—W/4Ã—C
    â†“                          â”‚           â†“
PatchMerge â†’ H/8Ã—W/8Ã—2C      â”€â”¼â”€â†’ Skip â”€â†’ Swin Ã— 2 â†’ H/4Ã—W/4Ã—C
    â†“                          â”‚
Swin Ã— 2    â†’ H/8Ã—W/8Ã—2C     â”€â”¼â”€â†’ Skip â”€â†’ PatchExpand â†’ H/8Ã—W/8Ã—2C
    â†“                          â”‚           â†“
PatchMerge â†’ H/16Ã—W/16Ã—4C    â”€â”¼â”€â†’ Skip â”€â†’ Swin Ã— 2 â†’ H/8Ã—W/8Ã—2C
    â†“                          â”‚
Swin Ã— 6  â†’ H/16Ã—W/16Ã—4C     â”€â”˜           â†“
    â†“                                  Output (HÃ—WÃ—C)
PatchMerge â†’ H/32Ã—W/32Ã—8C
    â†“
Swin Ã— 2 (Bottleneck)
```

### PyTorchå®ç°

```python
class SwinTransformerBlock(nn.Module):
    """Swin Transformer Block"""
    def __init__(self, dim, num_heads, window_size=7, shift_size=0):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        
        # è§„èŒƒåŒ–
        self.norm1 = nn.LayerNorm(dim)
        
        # Window Attention
        self.attn = WindowAttention(
            dim,
            window_size=(window_size, window_size),
            num_heads=num_heads
        )
        
        # MLP
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = Mlp(dim, int(dim * 4))
    
    def forward(self, x, H, W):
        B, L, C = x.shape
        assert L == H * W
        
        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)
        
        # Cyclic shift
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x
        
        # Partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # (B*num_windows, M, M, C)
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)
        
        # Window Attention
        attn_windows = self.attn(x_windows)
        
        # Merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)
        
        # Reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        
        x = x.view(B, H * W, C)
        x = shortcut + x
        
        # MLP
        x = x + self.mlp(self.norm2(x))
        
        return x


class PatchMerging(nn.Module):
    """ä¸‹é‡‡æ ·æ¨¡å—"""
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = nn.LayerNorm(4 * dim)
    
    def forward(self, x, H, W):
        B, L, C = x.shape
        x = x.view(B, H, W, C)
        
        # 2Ã—2é‚»åŸŸæ‹¼æ¥
        x0 = x[:, 0::2, 0::2, :]  # B, H/2, W/2, C
        x1 = x[:, 1::2, 0::2, :]
        x2 = x[:, 0::2, 1::2, :]
        x3 = x[:, 1::2, 1::2, :]
        x = torch.cat([x0, x1, x2, x3], dim=-1)  # B, H/2, W/2, 4C
        
        x = x.view(B, -1, 4 * C)
        x = self.norm(x)
        x = self.reduction(x)  # B, H/2*W/2, 2C
        
        return x


class PatchExpanding(nn.Module):
    """ä¸Šé‡‡æ ·æ¨¡å—ï¼ˆè§£ç å™¨ï¼‰"""
    def __init__(self, dim, dim_scale=2):
        super().__init__()
        self.dim = dim
        self.expand = nn.Linear(dim, 2 * dim, bias=False)
        self.norm = nn.LayerNorm(dim // dim_scale)
    
    def forward(self, x, H, W):
        B, L, C = x.shape
        x = self.expand(x)  # B, L, 2C
        
        x = x.view(B, H, W, 2 * C)
        x = x.view(B, H, W, 2, 2, C // 2).permute(0, 1, 3, 2, 4, 5).contiguous()
        x = x.view(B, H * 2, W * 2, C // 2)
        x = x.view(B, -1, C // 2)
        
        x = self.norm(x)
        return x


class SwinUNet(nn.Module):
    def __init__(self, img_size=224, num_classes=2, embed_dim=96, depths=[2,2,6,2]):
        super().__init__()
        
        ### ç¼–ç å™¨ ###
        self.patch_embed = PatchEmbed(embed_dim=embed_dim)
        
        # Stage 1
        self.encoder1 = nn.ModuleList([
            SwinTransformerBlock(
                dim=embed_dim,
                num_heads=3,
                window_size=7,
                shift_size=0 if i % 2 == 0 else 3
            ) for i in range(depths[0])
        ])
        
        # Stage 2
        self.down1 = PatchMerging(embed_dim)
        self.encoder2 = nn.ModuleList([
            SwinTransformerBlock(
                dim=embed_dim * 2,
                num_heads=6,
                window_size=7,
                shift_size=0 if i % 2 == 0 else 3
            ) for i in range(depths[1])
        ])
        
        # Stage 3
        self.down2 = PatchMerging(embed_dim * 2)
        self.encoder3 = nn.ModuleList([
            SwinTransformerBlock(
                dim=embed_dim * 4,
                num_heads=12,
                window_size=7,
                shift_size=0 if i % 2 == 0 else 3
            ) for i in range(depths[2])
        ])
        
        # Bottleneck (Stage 4)
        self.down3 = PatchMerging(embed_dim * 4)
        self.bottleneck = nn.ModuleList([
            SwinTransformerBlock(
                dim=embed_dim * 8,
                num_heads=24,
                window_size=7,
                shift_size=0 if i % 2 == 0 else 3
            ) for i in range(depths[3])
        ])
        
        ### è§£ç å™¨ ###
        self.up3 = PatchExpanding(embed_dim * 8, dim_scale=2)
        self.decoder3 = nn.ModuleList([
            SwinTransformerBlock(
                dim=embed_dim * 4,
                num_heads=12,
                window_size=7,
                shift_size=0 if i % 2 == 0 else 3
            ) for i in range(depths[2])
        ])
        
        # å…¶ä»–è§£ç å™¨å±‚ï¼ˆçœç•¥ç±»ä¼¼ä»£ç ï¼‰
        # ...
        
        # è¾“å‡º
        self.output = nn.Conv2d(embed_dim, num_classes, 1)
    
    def forward(self, x):
        # ç¼–ç å™¨
        x, H, W = self.patch_embed(x)
        
        # Stage 1
        for blk in self.encoder1:
            x = blk(x, H, W)
        skip1 = x
        
        # Stage 2
        x = self.down1(x, H, W)
        H, W = H // 2, W // 2
        for blk in self.encoder2:
            x = blk(x, H, W)
        skip2 = x
        
        # Stage 3
        x = self.down2(x, H, W)
        H, W = H // 2, W // 2
        for blk in self.encoder3:
            x = blk(x, H, W)
        skip3 = x
        
        # Bottleneck
        x = self.down3(x, H, W)
        H, W = H // 2, W // 2
        for blk in self.bottleneck:
            x = blk(x, H, W)
        
        # è§£ç å™¨ï¼ˆå¯¹ç§°ï¼‰
        x = self.up3(x, H, W)
        H, W = H * 2, W * 2
        x = x + skip3  # Skip connection
        for blk in self.decoder3:
            x = blk(x, H, W)
        
        # å…¶ä»–è§£ç å™¨å±‚...
        
        # è¾“å‡º
        x = x.view(-1, H, W, self.embed_dim).permute(0, 3, 1, 2)
        out = self.output(x)
        return out
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### Synapse Multi-organæ•°æ®é›†

| æ–¹æ³• | Dice | HD95 | å‚æ•°é‡ | GFLOPs |
|------|------|------|--------|--------|
| UNet | 76.85 | 39.70 | 31M | 54 |
| TransUNet | 81.87 | 28.78 | 105M | 200 |
| **Swin-UNet** | **83.24** | **25.44** | **27M** | **47** |

**å…³é”®ä¼˜åŠ¿**ï¼š
- âœ… **ç²¾åº¦æœ€é«˜**ï¼šDice 83.24%ï¼ˆ+1.4% vs. TransUNetï¼‰
- âœ… **å‚æ•°æœ€å°‘**ï¼š27Mï¼ˆä»…ä¸ºTransUNetçš„26%ï¼‰
- âœ… **é€Ÿåº¦æœ€å¿«**ï¼š47 GFLOPsï¼ˆTransUNetçš„24%ï¼‰

### å„å™¨å®˜åˆ†å‰²ç»“æœ

| å™¨å®˜ | UNet | TransUNet | **Swin-UNet** | æå‡ |
|------|------|-----------|--------------|------|
| ä¸»åŠ¨è„‰ | 87.23 | 90.75 | **92.18** | +1.4% |
| èƒ†å›Š | 68.60 | 77.42 | **80.35** | +2.9% |
| å·¦è‚¾ | 84.18 | 88.31 | **89.76** | +1.5% |
| èƒ°è…º | 56.45 | 70.84 | **75.21** | +4.4% |

**åˆ†æ**ï¼š
- å°å™¨å®˜ï¼ˆèƒ†å›Šã€èƒ°è…ºï¼‰æå‡æ›´æ˜æ˜¾
- å¤§å™¨å®˜ï¼ˆä¸»åŠ¨è„‰ã€è‚¾è„ï¼‰ä¹Ÿæœ‰ç¨³å®šæå‡

---

## ğŸ’¡ Swin-UNetçš„ä¼˜åŠ¿

### 1. è®¡ç®—æ•ˆç‡

```
512Ã—512å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼š

TransUNetï¼š
- Tokenæ•°é‡ï¼š64Ã—64 = 4096
- æ³¨æ„åŠ›å¤æ‚åº¦ï¼šO(4096^2) â‰ˆ 16.8M
- æ¨ç†æ—¶é—´ï¼šçº¦150msï¼ˆV100 GPUï¼‰

Swin-UNetï¼š
- Tokenæ•°é‡ï¼š64Ã—64 = 4096
- çª—å£å¤§å°ï¼š7Ã—7 = 49
- æ³¨æ„åŠ›å¤æ‚åº¦ï¼šO(4096 Ã— 49) â‰ˆ 0.2M
- æ¨ç†æ—¶é—´ï¼šçº¦60msï¼ˆV100 GPUï¼‰

åŠ é€Ÿæ¯”ï¼š2.5Ã—
```

### 2. å±‚çº§åŒ–ç‰¹å¾

```
Swin-UNetçš„å¤šå°ºåº¦ç‰¹å¾ï¼š
- Stage 1ï¼šH/4Ã—W/4ï¼ˆé«˜åˆ†è¾¨ç‡ï¼Œç»†èŠ‚ä¸°å¯Œï¼‰
- Stage 2ï¼šH/8Ã—W/8ï¼ˆä¸­åˆ†è¾¨ç‡ï¼Œè¾¹ç•Œä¿¡æ¯ï¼‰
- Stage 3ï¼šH/16Ã—W/16ï¼ˆä½åˆ†è¾¨ç‡ï¼Œè¯­ä¹‰ä¿¡æ¯ï¼‰
- Stage 4ï¼šH/32Ã—W/32ï¼ˆå…¨å±€ä¸Šä¸‹æ–‡ï¼‰

ä¼˜åŠ¿ï¼š
âœ“ ç±»ä¼¼CNNçš„ç‰¹å¾é‡‘å­—å¡”
âœ“ ä¸åŒå°ºåº¦ç‰¹å¾è‡ªç„¶èåˆ
âœ“ é€‚é…å¤šå°ºåº¦ç›®æ ‡
```

### 3. å…¨å±€æ„Ÿå—é‡

```
é€šè¿‡shifted windowsï¼š
Layer 1ï¼ˆå¸¸è§„çª—å£ï¼‰ï¼šå±€éƒ¨æ„Ÿå—é‡ = 7Ã—7
Layer 2ï¼ˆç§»ä½çª—å£ï¼‰ï¼šè·¨çª—å£äº¤äº’
Layer 3ï¼ˆå¸¸è§„çª—å£ï¼‰ï¼šæ„Ÿå—é‡æ‰©å¤§
...
å †å 12å±‚ï¼šå®ç°å…¨å±€æ„Ÿå—é‡

æ•ˆæœï¼š
âœ“ æ—¢æœ‰å±€éƒ¨ç»†èŠ‚ï¼ˆWindow Attentionï¼‰
âœ“ åˆæœ‰å…¨å±€è¯­ä¹‰ï¼ˆShifted Windowsï¼‰
```

---

## ğŸ“ è®­ç»ƒæŠ€å·§

### 1. çª—å£å¤§å°è°ƒä¼˜

```python
# çª—å£å¤§å°å½±å“æ€§èƒ½
window_sizes = [4, 7, 14]
results = {}

for ws in window_sizes:
    model = SwinUNet(window_size=ws)
    dice = train_and_evaluate(model)
    results[ws] = dice

# å…¸å‹ç»“æœï¼š
# window_size=4: Dice=81.5% (å°çª—å£ï¼Œå±€éƒ¨æ€§å¼º)
# window_size=7: Dice=83.2% (æœ€ä½³å¹³è¡¡)
# window_size=14: Dice=82.1% (å¤§çª—å£ï¼Œè®¡ç®—é‡å¤§)
```

### 2. æ•°æ®å¢å¼º

```python
# Swin-UNetå¯¹æ—‹è½¬æ•æ„Ÿï¼ˆä½ç½®ç¼–ç ï¼‰
# éœ€è¦æ—‹è½¬å¢å¼ºæ¥æå‡æ³›åŒ–æ€§

transforms = A.Compose([
    A.RandomRotate90(p=0.5),
    A.Rotate(limit=30, p=0.8),
    A.ShiftScaleRotate(
        shift_limit=0.1,
        scale_limit=0.2,
        rotate_limit=20,
        p=0.8
    ),
    # ... å…¶ä»–å¢å¼º
])
```

### 3. å­¦ä¹ ç‡è°ƒåº¦

```python
# Swin-UNetä½¿ç”¨AdamW + Cosine Annealing
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-4,
    weight_decay=0.05
)

scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=150,
    eta_min=1e-6
)
```

---

## ğŸ“– æ€»ç»“

### Swin-UNetçš„æ ¸å¿ƒè´¡çŒ®

1. **Window Attentionç ´è§£æ•ˆç‡å›°å±€**
   - å¤æ‚åº¦ï¼š\( O(N^2) \rightarrow O(N) \)
   - 2.5Ã—æ¨ç†åŠ é€Ÿ

2. **Shifted Windowså®ç°å…¨å±€å»ºæ¨¡**
   - äº¤æ›¿ä½¿ç”¨å¸¸è§„/ç§»ä½çª—å£
   - ä¿ç•™Transformerå…¨å±€æ„Ÿå—é‡ä¼˜åŠ¿

3. **å±‚çº§åŒ–æ¶æ„**
   - ç±»ä¼¼CNNçš„å¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”
   - æ›´è‡ªç„¶çš„ç¼–ç å™¨-è§£ç å™¨èåˆ

4. **SOTAæ€§èƒ½ + é«˜æ•ˆç‡**
   - Dice: 83.24%ï¼ˆæœ€é«˜ï¼‰
   - å‚æ•°ï¼š27Mï¼ˆTransUNetçš„26%ï¼‰
   - é€Ÿåº¦ï¼š2.5Ã—åŠ é€Ÿ

### é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ¨èåº¦ | åŸå›  |
|------|-------|------|
| é«˜åˆ†è¾¨ç‡å›¾åƒ | âœ…âœ…âœ… | çº¿æ€§å¤æ‚åº¦ |
| å®æ—¶åº”ç”¨ | âœ…âœ… | é€Ÿåº¦å¿« |
| èµ„æºå—é™ | âœ…âœ… | å‚æ•°å°‘ |
| å¤šå°ºåº¦ç›®æ ‡ | âœ…âœ…âœ… | å±‚çº§ç‰¹å¾ |
| æå°æ•°æ®é›† | âš ï¸ | éœ€é¢„è®­ç»ƒ |

**ä¸‹ä¸€ç¯‡é¢„å‘Š**ï¼š[SAMä¸MedSAMï¼šåŸºç¡€æ¨¡å‹çš„åŒ»å­¦åº”ç”¨](/2025/03/05/sam-segment-anything/) - æ¢ç´¢Segment Anything Modelå¦‚ä½•é€šè¿‡promptå®ç°zero-shotåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚

---

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡
1. [Swin-UNet] Cao, H., et al. (2021). Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation. *arXiv*.
2. [Swin Transformer] Liu, Z., et al. (2021). Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. *ICCV*.
3. [TransUNet] Chen, J., et al. (2021). TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation. *arXiv*.

### ä»£ç å®ç°
- [Swin-UNetå®˜æ–¹](https://github.com/HuCaoFighting/Swin-Unet) - PyTorchå®ç°
- [Swin Transformerå®˜æ–¹](https://github.com/microsoft/Swin-Transformer) - Microsoftå®˜æ–¹ä»£ç 
- [MONAI](https://github.com/Project-MONAI/MONAI) - åŒ»å­¦å›¾åƒæ¡†æ¶ï¼ŒåŒ…å«Swin-UNet

---

## ğŸ”— ç³»åˆ—æ–‡ç« å¯¼èˆª

1. [FCNä¸UNetï¼šåŒ»å­¦åˆ†å‰²çš„å¥ åŸºä¹‹ä½œ](/2025/02/01/fcn-unet-foundation/)
2. [V-Netï¼š3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„çªç ´](/2025/02/05/vnet-3d-segmentation/)
3. [Attention UNetï¼šæ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥](/2025/02/10/attention-unet/)
4. [UNet++/UNet 3+ï¼šå¯†é›†è¿æ¥çš„åŠ›é‡](/2025/02/15/unet-plus-series/)
5. [TransUNetï¼šCNNä¸Transformerçš„èåˆ](/2025/02/20/transunet-hybrid-architecture/)
6. ğŸ“ **Swin-UNetï¼šå±‚çº§åŒ–Transformer**ï¼ˆæœ¬æ–‡ï¼‰
7. [SAMä¸MedSAMï¼šåŸºç¡€æ¨¡å‹çš„åŒ»å­¦åº”ç”¨](/2025/03/05/sam-segment-anything/)
8. [nnU-Netï¼šè‡ªé€‚åº”åŒ»å­¦åˆ†å‰²æ¡†æ¶](/2025/03/15/nnunet-self-configuring-framework/)

---

*æœ¬æ–‡æ·±å…¥è§£æäº†Swin-UNetå¦‚ä½•é€šè¿‡shifted windowså’Œå±‚çº§æ¶æ„å®ç°é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œåœ¨ä¿æŒå…¨å±€å»ºæ¨¡èƒ½åŠ›çš„åŒæ—¶å¤§å¹…é™ä½è®¡ç®—å¤æ‚åº¦ã€‚ä¸‹ä¸€ç¯‡å°†ä»‹ç»SAMå¦‚ä½•é€šè¿‡promptå®ç°é€šç”¨åˆ†å‰²ã€‚*

