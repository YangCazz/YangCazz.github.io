---
layout: post
title: "SAMä¸MedSAMï¼šåŸºç¡€æ¨¡å‹å¼•é¢†åŒ»å­¦åˆ†å‰²æ–°èŒƒå¼"
date: 2022-10-01 10:00:00 +0800
categories: [åŒ»å­¦å½±åƒ, å›¾åƒåˆ†å‰²]
tags: [UNet, åŒ»å­¦å›¾åƒ, Foundation Model]
excerpt: "æ·±å…¥æ¢è®¨Meta AIçš„Segment Anything Modelï¼ˆSAMï¼‰å¦‚ä½•é€šè¿‡promptæœºåˆ¶å®ç°é€šç”¨åˆ†å‰²ï¼Œä»¥åŠMedSAMå¦‚ä½•å°†å…¶æˆåŠŸè¿ç§»åˆ°åŒ»å­¦å½±åƒé¢†åŸŸã€‚"
author: YangCazz
math: true
---

## ğŸ“‹ å¼•è¨€

åœ¨å‰é¢çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å„ç§ä¸“é—¨ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²è®¾è®¡çš„ç½‘ç»œï¼šä»[UNet](/2025/02/01/fcn-unet-foundation/)çš„Uå‹ç»“æ„ï¼Œåˆ°[Transformer](/2025/02/20/transunet-hybrid-architecture/)çš„å…¨å±€å»ºæ¨¡ã€‚è¿™äº›æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†éƒ½å­˜åœ¨ä¸€ä¸ªå…±åŒé—®é¢˜ï¼š

**éœ€è¦é’ˆå¯¹æ¯ä¸ªä»»åŠ¡å•ç‹¬è®­ç»ƒ**

```
ä¼ ç»Ÿæ–¹æ³•çš„å›°å¢ƒï¼š

ä»»åŠ¡1ï¼šè‚è„åˆ†å‰²
â†’ æ”¶é›†è‚è„æ ‡æ³¨æ•°æ®
â†’ è®­ç»ƒUNet/TransUNet
â†’ ä»…èƒ½åˆ†å‰²è‚è„

ä»»åŠ¡2ï¼šè‚ºéƒ¨åˆ†å‰²  
â†’ é‡æ–°æ”¶é›†è‚ºéƒ¨æ•°æ®
â†’ é‡æ–°è®­ç»ƒæ¨¡å‹
â†’ ä»…èƒ½åˆ†å‰²è‚ºéƒ¨

é—®é¢˜ï¼š
âœ— æ¯ä¸ªä»»åŠ¡éœ€è¦å¤§é‡æ ‡æ³¨
âœ— æ— æ³•åˆ©ç”¨å·²å­¦çŸ¥è¯†
âœ— æ³›åŒ–èƒ½åŠ›æœ‰é™
```

**SAMï¼ˆSegment Anything Modelï¼Œ2023ï¼‰** æå‡ºäº†é©å‘½æ€§çš„æƒ³æ³•ï¼š

> **ä¸€ä¸ªæ¨¡å‹ï¼Œåˆ†å‰²ä¸€åˆ‡**

é€šè¿‡**Promptable Segmentation**ï¼ˆå¯æç¤ºåˆ†å‰²ï¼‰ï¼ŒSAMå®ç°ï¼š
- âœ… **Zero-shot**ï¼šæ— éœ€è®­ç»ƒå³å¯åˆ†å‰²æ–°ç±»åˆ«
- âœ… **äº¤äº’å¼**ï¼šé€šè¿‡ç‚¹å‡»ã€æ¡†é€‰ã€æ–‡æœ¬ç­‰æ–¹å¼æŒ‡å®šç›®æ ‡
- âœ… **é€šç”¨æ€§**ï¼šä¸€ä¸ªæ¨¡å‹å¤„ç†æ‰€æœ‰åˆ†å‰²ä»»åŠ¡

**MedSAM**åˆ™å°†SAMæˆåŠŸè¿ç§»åˆ°åŒ»å­¦é¢†åŸŸï¼Œæˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°èŒƒå¼ã€‚

---

## ğŸ¯ SAMï¼šæ ¸å¿ƒæ€æƒ³

### è®ºæ–‡ä¿¡æ¯
- **æ ‡é¢˜**: Segment Anything
- **ä½œè€…**: Alexander Kirillov, et al. (Meta AI Research)
- **å‘è¡¨**: ICCV 2023
- **è®ºæ–‡é“¾æ¥**: [arXiv:2304.02643](https://arxiv.org/abs/2304.02643)
- **å®˜æ–¹ä»£ç **: [GitHub](https://github.com/facebookresearch/segment-anything)
- **æ•°æ®é›†**: SA-1Bï¼ˆ11Må›¾åƒï¼Œ1.1B maskï¼‰

### ä»€ä¹ˆæ˜¯Promptable Segmentationï¼Ÿ

**ä¼ ç»Ÿåˆ†å‰²**ï¼šè¾“å…¥å›¾åƒ â†’ è¾“å‡ºå›ºå®šç±»åˆ«çš„mask

**Promptableåˆ†å‰²**ï¼šè¾“å…¥å›¾åƒ + **Prompt** â†’ è¾“å‡ºå¯¹åº”çš„mask

**Promptç±»å‹**ï¼š

1. **Point Prompt**ï¼ˆç‚¹æç¤ºï¼‰
   ```
   ç”¨æˆ·ç‚¹å‡»ç›®æ ‡ â†’ åˆ†å‰²è¯¥ç›®æ ‡
   
   ç¤ºä¾‹ï¼šç‚¹å‡»å¿ƒè„ â†’ åˆ†å‰²å¿ƒè„
         ç‚¹å‡»è‚¿ç˜¤ â†’ åˆ†å‰²è‚¿ç˜¤
   ```

2. **Box Prompt**ï¼ˆæ¡†æç¤ºï¼‰
   ```
   ç”¨æˆ·æ¡†é€‰åŒºåŸŸ â†’ åˆ†å‰²åŒºåŸŸå†…ç›®æ ‡
   
   ç¤ºä¾‹ï¼šæ¡†é€‰è‚è„ â†’ ç²¾ç¡®åˆ†å‰²è‚è„è¾¹ç•Œ
   ```

3. **Mask Prompt**ï¼ˆmaskæç¤ºï¼‰
   ```
   ç”¨æˆ·æä¾›ç²—ç³™mask â†’ ç²¾ç»†åŒ–åˆ†å‰²
   
   ç¤ºä¾‹ï¼šæ¶‚é¸¦æ ‡æ³¨ â†’ ç²¾ç¡®åˆ†å‰²
   ```

4. **Text Prompt**ï¼ˆæ–‡æœ¬æç¤ºï¼ŒSAMä¸ç›´æ¥æ”¯æŒï¼‰
   ```
   ç”¨æˆ·è¾“å…¥"liver" â†’ åˆ†å‰²è‚è„
   ```

### SAMæ¶æ„

SAM = **Image Encoder** + **Prompt Encoder** + **Mask Decoder**

```
å›¾åƒè¾“å…¥ (1024Ã—1024Ã—3)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image Encoder (ViT-H) â”‚
â”‚  - Vision Transformer  â”‚
â”‚  - è¾“å‡ºï¼š256Ã—64Ã—64     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    å›¾åƒåµŒå…¥
        â†“
Promptè¾“å…¥  â†’  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
(ç‚¹/æ¡†/mask)   â”‚  Prompt Encoder     â”‚
               â”‚  - ç‚¹ï¼šä½ç½®ç¼–ç      â”‚
               â”‚  - æ¡†ï¼šåµŒå…¥å‘é‡     â”‚
               â”‚  - Maskï¼šå·ç§¯ç¼–ç    â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
                  PromptåµŒå…¥
                       â†“
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   Mask Decoder      â”‚
               â”‚   - Transformer     â”‚
               â”‚   - äº¤å‰æ³¨æ„åŠ›      â”‚
               â”‚   - è¾“å‡ºå¤šä¸ªmask    â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
               Masks + IoU Scores
            (å¯èƒ½æœ‰å¤šä¸ªå€™é€‰)
```

### å…³é”®ç»„ä»¶

#### 1. Image Encoder

```python
# ä½¿ç”¨ViT-Hï¼ˆHugeï¼‰ä½œä¸ºå›¾åƒç¼–ç å™¨
class ImageEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # Vision Transformer (ViT-H/16)
        self.vit = VisionTransformer(
            img_size=1024,
            patch_size=16,
            embed_dim=1280,
            depth=32,
            num_heads=16
        )
    
    def forward(self, x):
        # x: (B, 3, 1024, 1024)
        features = self.vit(x)  # (B, 256, 64, 64)
        return features
```

**ç‰¹ç‚¹**ï¼š
- è¾“å…¥å›ºå®š1024Ã—1024ï¼ˆé¢„å¤„ç†æ—¶resizeï¼‰
- è¾“å‡º256é€šé“çš„64Ã—64ç‰¹å¾å›¾
- å‚æ•°é‡ï¼šçº¦630Mï¼ˆå SAMæ€»å‚æ•°çš„99%ï¼‰

#### 2. Prompt Encoder

```python
class PromptEncoder(nn.Module):
    def __init__(self, embed_dim=256):
        super().__init__()
        self.embed_dim = embed_dim
        
        # ç‚¹æç¤ºç¼–ç 
        self.point_embeddings = nn.Embedding(2, embed_dim)  # å‰æ™¯/èƒŒæ™¯ç‚¹
        
        # æ¡†æç¤ºç¼–ç 
        self.box_embeddings = nn.Embedding(4, embed_dim)  # å·¦ä¸Šã€å³ä¸‹è§’
        
        # Maskæç¤ºç¼–ç 
        self.mask_encoder = nn.Sequential(
            nn.Conv2d(1, embed_dim // 4, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(embed_dim // 4, embed_dim, 3, padding=1)
        )
    
    def forward(self, points=None, boxes=None, masks=None):
        sparse_embeddings = []
        
        # ç¼–ç ç‚¹
        if points is not None:
            point_embeddings = self.point_embeddings(points[:, :, 2])  # å‰æ™¯=1,èƒŒæ™¯=0
            point_embeddings += self._get_positional_encoding(points[:, :, :2])
            sparse_embeddings.append(point_embeddings)
        
        # ç¼–ç æ¡†
        if boxes is not None:
            box_embeddings = self._encode_boxes(boxes)
            sparse_embeddings.append(box_embeddings)
        
        # ç¼–ç mask
        dense_embeddings = None
        if masks is not None:
            dense_embeddings = self.mask_encoder(masks)
        
        return sparse_embeddings, dense_embeddings
    
    def _get_positional_encoding(self, coords):
        """ä½ç½®ç¼–ç ï¼šå°†(x,y)åæ ‡ç¼–ç ä¸ºé«˜ç»´å‘é‡"""
        # ä½¿ç”¨æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç 
        # ...
        return pos_encoding
```

**Promptç¼–ç ç­–ç•¥**ï¼š
- **ç¨€ç–Prompt**ï¼ˆç‚¹ã€æ¡†ï¼‰ï¼šä½¿ç”¨ä½ç½®ç¼–ç  + å­¦ä¹ åµŒå…¥
- **å¯†é›†Prompt**ï¼ˆmaskï¼‰ï¼šä½¿ç”¨å·ç§¯ç½‘ç»œç¼–ç 

#### 3. Mask Decoder

```python
class MaskDecoder(nn.Module):
    def __init__(self, transformer_dim=256, num_mask_tokens=4):
        super().__init__()
        
        # Mask tokensï¼ˆå¯å­¦ä¹ çš„queryï¼‰
        self.mask_tokens = nn.Embedding(num_mask_tokens, transformer_dim)
        
        # Transformerè§£ç å™¨
        self.transformer = nn.ModuleList([
            TwoWayTransformer(
                depth=2,
                embedding_dim=transformer_dim,
                num_heads=8
            )
        ])
        
        # è¾“å‡ºMLP
        self.output_upscaling = nn.Sequential(
            nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, 2, 2),
            nn.LayerNorm(...),
            nn.GELU(),
            nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, 2, 2),
            nn.GELU()
        )
        
        self.output_hypernetworks_mlps = nn.ModuleList([
            MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3)
            for _ in range(num_mask_tokens)
        ])
        
        # IoUé¢„æµ‹å¤´
        self.iou_prediction_head = MLP(transformer_dim, 256, num_mask_tokens, 3)
    
    def forward(self, image_embeddings, sparse_prompt_embeddings, dense_prompt_embeddings):
        # image_embeddings: (B, 256, 64, 64)
        # sparse_prompt_embeddings: [(B, N, 256), ...]
        
        # å‡†å¤‡è¾“å‡ºtokens
        output_tokens = self.mask_tokens.weight.unsqueeze(0).repeat(B, 1, 1)
        tokens = torch.cat([output_tokens, sparse_prompt_embeddings], dim=1)
        
        # å°†å›¾åƒåµŒå…¥å±•å¹³
        src = image_embeddings.flatten(2).permute(0, 2, 1)  # (B, 4096, 256)
        
        # Transformerè§£ç 
        hs, src = self.transformer[0](src, tokens)
        
        # é¢„æµ‹maskå’ŒIoU
        masks = []
        iou_pred = self.iou_prediction_head(hs[:, :self.num_mask_tokens, :])
        
        # ä¸Šé‡‡æ ·ç‰¹å¾
        src = src.transpose(1, 2).view(B, 256, 64, 64)
        upscaled_embedding = self.output_upscaling(src)  # (B, 32, 256, 256)
        
        # ä¸ºæ¯ä¸ªmask tokenç”Ÿæˆmask
        for i in range(self.num_mask_tokens):
            masks.append(
                self.output_hypernetworks_mlps[i](hs[:, i, :]) @ upscaled_embedding.view(B, 32, -1)
            )
        
        masks = torch.stack(masks, dim=1).view(B, -1, 256, 256)
        
        return masks, iou_pred
```

**å…³é”®è®¾è®¡**ï¼š
- **å¤šmaskè¾“å‡º**ï¼šåŒæ—¶é¢„æµ‹å¤šä¸ªå€™é€‰maskï¼ˆé€šå¸¸3ä¸ªï¼‰
- **IoUé¢„æµ‹**ï¼šä¸ºæ¯ä¸ªmaské¢„æµ‹è´¨é‡åˆ†æ•°
- **æœ€ä¼˜maské€‰æ‹©**ï¼šæ ¹æ®IoUåˆ†æ•°é€‰æ‹©æœ€ä½³mask

### è®­ç»ƒç­–ç•¥

#### SA-1Bæ•°æ®é›†

**è§„æ¨¡**ï¼š
- å›¾åƒæ•°é‡ï¼š11Mï¼ˆ1100ä¸‡ï¼‰
- Maskæ•°é‡ï¼š1.1Bï¼ˆ11äº¿ï¼‰
- å¹³å‡æ¯å¼ å›¾100ä¸ªmask

**æ„å»ºæµç¨‹**ï¼ˆæ•°æ®é£è½®ï¼‰ï¼š

```
é˜¶æ®µ1ï¼šè¾…åŠ©æ ‡æ³¨ï¼ˆAssisted-manualï¼‰
â†’ ä¸“ä¸šæ ‡æ³¨å‘˜ä½¿ç”¨SAMè¾…åŠ©æ ‡æ³¨
â†’ æ”¶é›†4.3M maskï¼ˆ120Kå›¾åƒï¼‰

é˜¶æ®µ2ï¼šåŠè‡ªåŠ¨æ ‡æ³¨ï¼ˆSemi-automaticï¼‰
â†’ SAMè‡ªåŠ¨å»ºè®®mask
â†’ æ ‡æ³¨å‘˜å®¡æ ¸å’Œä¿®æ­£
â†’ æ”¶é›†5.9M maskï¼ˆ180Kå›¾åƒï¼‰

é˜¶æ®µ3ï¼šå…¨è‡ªåŠ¨æ ‡æ³¨ï¼ˆFully automaticï¼‰
â†’ SAMè‡ªåŠ¨ç”Ÿæˆmask
â†’ è‡ªåŠ¨è¿‡æ»¤ä½è´¨é‡mask
â†’ æ”¶é›†1.1B maskï¼ˆ11Må›¾åƒï¼‰
```

#### æŸå¤±å‡½æ•°

$$
\mathcal{L} = \mathcal{L}_{\text{Focal}} + \mathcal{L}_{\text{Dice}} + \mathcal{L}_{\text{IoU}}
$$

**Focal Loss**ï¼šå¤„ç†å‰æ™¯/èƒŒæ™¯ä¸å¹³è¡¡

$$
\mathcal{L}_{\text{Focal}} = -\alpha (1 - p_t)^\gamma \log(p_t)
$$

**Dice Loss**ï¼šç›´æ¥ä¼˜åŒ–Diceç³»æ•°

$$
\mathcal{L}_{\text{Dice}} = 1 - \frac{2|P \cap G|}{|P| + |G|}
$$

**IoU Loss**ï¼šè¾…åŠ©IoUé¢„æµ‹å¤´

$$
\mathcal{L}_{\text{IoU}} = \text{MSE}(\text{IoU}_{\text{pred}}, \text{IoU}_{\text{true}})
$$

---

## ğŸ¥ MedSAMï¼šåŒ»å­¦é¢†åŸŸçš„SAM

### è®ºæ–‡ä¿¡æ¯
- **æ ‡é¢˜**: Segment Anything in Medical Images
- **ä½œè€…**: Jun Ma, et al. (University of Toronto)
- **å‘è¡¨**: Nature Communications 2024
- **è®ºæ–‡é“¾æ¥**: [arXiv:2304.12306](https://arxiv.org/abs/2304.12306)
- **å®˜æ–¹ä»£ç **: [GitHub](https://github.com/bowang-lab/MedSAM)

### ä¸ºä»€ä¹ˆéœ€è¦MedSAMï¼Ÿ

**SAMåœ¨åŒ»å­¦å›¾åƒä¸Šçš„é—®é¢˜**ï¼š

```
æµ‹è¯•SAMï¼ˆé›¶æ ·æœ¬ï¼‰åœ¨åŒ»å­¦å›¾åƒä¸Šï¼š

æ•°æ®é›†ï¼šSynapse Multi-organ CT
ç»“æœï¼š
- è‚è„ Dice: 0.42ï¼ˆUNet: 0.94ï¼‰
- èƒ°è…º Dice: 0.18ï¼ˆUNet: 0.70ï¼‰
- å¹³å‡ Dice: 0.35ï¼ˆUNet: 0.85ï¼‰

é—®é¢˜ï¼š
âœ— SAMè®­ç»ƒæ•°æ®å…¨æ˜¯è‡ªç„¶å›¾åƒ
âœ— åŒ»å­¦å›¾åƒç‰¹æ€§ï¼ˆç°åº¦ã€å™ªå£°ã€æ¨¡æ€ï¼‰å®Œå…¨ä¸åŒ
âœ— Zero-shotæ³›åŒ–å¤±è´¥
```

**MedSAMçš„è§£å†³æ–¹æ¡ˆ**ï¼š

ä½¿ç”¨**åŒ»å­¦å›¾åƒæ•°æ®**fine-tune SAM

### MedSAMæ•°æ®é›†

**è§„æ¨¡**ï¼š
- å›¾åƒæ•°é‡ï¼š1.57M
- Maskæ•°é‡ï¼šçº¦10M
- æ¨¡æ€ï¼š10ç§ï¼ˆCTã€MRIã€è¶…å£°ã€X-rayã€çœ¼åº•ã€ç—…ç†ç­‰ï¼‰
- è§£å‰–ç»“æ„ï¼š30+ ç±»ï¼ˆå™¨å®˜ã€è‚¿ç˜¤ã€ç—…ç¶ï¼‰

**æ•°æ®æ¥æº**ï¼š
- å…¬å¼€æ•°æ®é›†ï¼šNCIã€TCIAã€Medical Segmentation Decathlonç­‰
- åˆä½œåŒ»é™¢ï¼šå¤šä¸­å¿ƒæ•°æ®

### MedSAMæ¶æ„

**ä¿®æ”¹**ï¼šä»…fine-tune SAMï¼Œæ¶æ„ä¸å˜

```python
# MedSAM = SAM + åŒ»å­¦å›¾åƒfine-tuning
model = SAM(
    image_encoder='vit_h',  # ä¿æŒViT-H
    prompt_encoder='default',  # ä¿æŒä¸å˜
    mask_decoder='default'  # ä¿æŒä¸å˜
)

# Fine-tuningç­–ç•¥
for name, param in model.named_parameters():
    if 'image_encoder' in name:
        param.requires_grad = True  # è§£å†»å›¾åƒç¼–ç å™¨
    else:
        param.requires_grad = False  # å†»ç»“å…¶ä»–éƒ¨åˆ†ï¼ˆåˆæœŸï¼‰
```

### è®­ç»ƒç­–ç•¥

```python
# é˜¶æ®µ1ï¼šä»…fine-tune Image Encoder
optimizer = torch.optim.AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=1e-4,
    weight_decay=0.01
)

for epoch in range(10):
    for images, masks, boxes in train_loader:
        # ä½¿ç”¨box promptè®­ç»ƒ
        pred_masks, iou_pred = model(images, boxes=boxes)
        loss = focal_dice_loss(pred_masks, masks)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# é˜¶æ®µ2ï¼šfine-tuneæ•´ä¸ªç½‘ç»œ
for param in model.parameters():
    param.requires_grad = True

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=5e-5,  # æ›´å°çš„å­¦ä¹ ç‡
    weight_decay=0.01
)

for epoch in range(10, 30):
    # ... è®­ç»ƒ
```

### æ€§èƒ½å¯¹æ¯”

#### å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²

| æ¨¡æ€ | ä»»åŠ¡ | SAM (Zero-shot) | **MedSAM** | UNet |
|------|------|-----------------|-----------|------|
| CT | è‚è„ | 0.42 | **0.92** | 0.94 |
| CT | èƒ°è…º | 0.18 | **0.68** | 0.70 |
| MRI | å¿ƒè„ | 0.35 | **0.88** | 0.90 |
| è¶…å£° | ç”²çŠ¶è…ºç»“èŠ‚ | 0.25 | **0.75** | 0.78 |
| X-ray | è‚ºéƒ¨ | 0.30 | **0.83** | 0.85 |
| çœ¼åº• | è§†ç›˜ | 0.50 | **0.91** | 0.92 |
| ç—…ç† | ç»†èƒæ ¸ | 0.40 | **0.82** | 0.84 |

**å…³é”®å‘ç°**ï¼š
- âœ… MedSAMæ¥è¿‘ä¸“ç”¨UNetçš„æ€§èƒ½
- âœ… **ä¸€ä¸ªæ¨¡å‹å¤„ç†æ‰€æœ‰æ¨¡æ€**ï¼ˆvs. æ¯ä¸ªä»»åŠ¡è®­ç»ƒä¸€ä¸ªUNetï¼‰
- âœ… å¯¹æ–°ç±»åˆ«æœ‰è‰¯å¥½æ³›åŒ–

#### Few-shotå­¦ä¹ 

```
åœºæ™¯ï¼šæ–°ä»»åŠ¡ï¼ˆæ–°å™¨å®˜/æ–°æ¨¡æ€ï¼‰ï¼Œä»…æœ‰å°‘é‡æ ‡æ³¨

å®éªŒï¼šä½¿ç”¨1ã€5ã€10ã€50ä¸ªæ ‡æ³¨æ ·æœ¬fine-tune

ç»“æœï¼ˆå¹³å‡Diceï¼‰ï¼š
æ ·æœ¬æ•° | SAM | MedSAM | UNet
  1    | 0.12 | 0.45  | 0.30
  5    | 0.25 | 0.62  | 0.55
 10    | 0.32 | 0.71  | 0.68
 50    | 0.40 | 0.80  | 0.82

è§‚å¯Ÿï¼š
- MedSAMåœ¨æå°‘æ ·æœ¬æ—¶ä¼˜åŠ¿å·¨å¤§ï¼ˆ+50% vs. SAMï¼‰
- æ¯”UNetæ›´é«˜æ•ˆï¼ˆ10æ ·æœ¬è¾¾åˆ°UNet 50æ ·æœ¬çš„æ€§èƒ½ï¼‰
```

---

## ğŸ’¡ SAM/MedSAMçš„ä¼˜åŠ¿ä¸å±€é™

### âœ… ä¼˜åŠ¿

#### 1. Zero/Few-shotèƒ½åŠ›

```
ä¼ ç»ŸUNetï¼š
ä»»åŠ¡Aï¼ˆè‚è„ï¼‰ â†’ æ”¶é›†1000ä¾‹ â†’ è®­ç»ƒ â†’ æ¨¡å‹A
ä»»åŠ¡Bï¼ˆè‚ºï¼‰   â†’ æ”¶é›†1000ä¾‹ â†’ è®­ç»ƒ â†’ æ¨¡å‹B

MedSAMï¼š
é¢„è®­ç»ƒ â†’ æ¨¡å‹
ä»»åŠ¡A â†’ 5ä¾‹fine-tune â†’ å®Œæˆ
ä»»åŠ¡B â†’ 5ä¾‹fine-tune â†’ å®Œæˆ
```

#### 2. äº¤äº’å¼åˆ†å‰²

```python
# ç”¨æˆ·äº¤äº’æµç¨‹
def interactive_segmentation(image, user_clicks):
    model.eval()
    
    # åˆå§‹ç‚¹å‡»
    points = user_clicks  # [(x1, y1, 1), ...]  1=å‰æ™¯,0=èƒŒæ™¯
    pred_mask, iou = model(image, points=points)
    
    # æ˜¾ç¤ºç»“æœç»™ç”¨æˆ·
    show_mask(pred_mask)
    
    # ç”¨æˆ·ä¿®æ­£ï¼šæ·»åŠ å‰æ™¯/èƒŒæ™¯ç‚¹
    while True:
        new_point = get_user_click()
        if new_point is None:
            break
        
        points.append(new_point)
        pred_mask, iou = model(image, points=points)
        show_mask(pred_mask)
    
    return pred_mask
```

**åº”ç”¨åœºæ™¯**ï¼š
- æ”¾å°„ç§‘åŒ»ç”Ÿå¿«é€Ÿæ ‡æ³¨
- ç—…ç†å­¦å®¶è¾…åŠ©è¯Šæ–­
- ç ”ç©¶äººå‘˜æ•°æ®å‡†å¤‡

#### 3. é€šç”¨æ€§

```
ä¸€ä¸ªMedSAMæ¨¡å‹æ”¯æŒï¼š
- 10+ åŒ»å­¦å›¾åƒæ¨¡æ€
- 30+ è§£å‰–ç»“æ„
- ç‚¹/æ¡†/maskç­‰å¤šç§prompt

vs.

ä¼ ç»Ÿæ–¹æ³•éœ€è¦50+ ä¸ªä¸“ç”¨æ¨¡å‹
```

### âŒ å±€é™

#### 1. è®¡ç®—èµ„æºéœ€æ±‚

```
MedSAMå‚æ•°é‡ï¼š636M
æ¨ç†æ—¶é—´ï¼šçº¦2s/å›¾ï¼ˆRTX 3090ï¼‰
GPUå†…å­˜ï¼šçº¦16GB

vs.

UNetå‚æ•°é‡ï¼š31M
æ¨ç†æ—¶é—´ï¼šçº¦50ms/å›¾
GPUå†…å­˜ï¼šçº¦2GB

é—®é¢˜ï¼š
âœ— ä¸´åºŠå®æ—¶åº”ç”¨å›°éš¾
âœ— è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æŒ‘æˆ˜
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- MobileSAMï¼ˆ5.7Må‚æ•°ï¼Œ60Ã—åŠ é€Ÿï¼‰
- FastSAMï¼ˆåŸºäºYOLOï¼Œå®æ—¶æ¨ç†ï¼‰

#### 2. ç²¾åº¦ä»æœ‰å·®è·

```
å¤æ‚ä»»åŠ¡ï¼ˆå¦‚å°å™¨å®˜ã€è¾¹ç•Œæ¨¡ç³Šï¼‰ï¼š
MedSAM Dice: 0.68-0.75
ä¸“ç”¨UNet Dice: 0.80-0.85

å·®è·ï¼šçº¦5-10%
```

#### 3. éœ€è¦Prompt

```
MedSAMä¸èƒ½ï¼š
- è¾“å…¥å›¾åƒ â†’ ç›´æ¥è¾“å‡ºæ‰€æœ‰å™¨å®˜åˆ†å‰²

éœ€è¦ï¼š
- æ‰‹å·¥ç‚¹å‡»/æ¡†é€‰æ¯ä¸ªç›®æ ‡
- æˆ–é¢„å…ˆæä¾›bounding box

è‡ªåŠ¨åŒ–ç¨‹åº¦ä½äºå…¨è‡ªåŠ¨åˆ†å‰²
```

---

## ğŸ“ å®ç”¨æŠ€å·§

### 1. Promptå·¥ç¨‹

```python
# ç­–ç•¥1ï¼šBox Promptæœ€ç¨³å®š
def get_box_prompt(mask_gt):
    """ä»ground truthæå–bounding box"""
    y, x = np.where(mask_gt > 0)
    x_min, x_max = x.min(), x.max()
    y_min, y_max = y.min(), y.max()
    return np.array([x_min, y_min, x_max, y_max])

# ç­–ç•¥2ï¼šå¤šç‚¹Promptå¢å¼ºé²æ£’æ€§
def get_multi_point_prompt(mask_gt, num_points=5):
    """åœ¨ç›®æ ‡åŒºåŸŸå†…é‡‡æ ·å¤šä¸ªå‰æ™¯ç‚¹"""
    y, x = np.where(mask_gt > 0)
    indices = np.random.choice(len(x), size=num_points, replace=False)
    points = np.stack([x[indices], y[indices], np.ones(num_points)], axis=1)
    return points

# ç­–ç•¥3ï¼šå‰æ™¯+èƒŒæ™¯ç‚¹
def get_fg_bg_points(mask_gt):
    """ç»“åˆå‰æ™¯å’ŒèƒŒæ™¯ç‚¹"""
    # å‰æ™¯ç‚¹
    y_fg, x_fg = np.where(mask_gt > 0)
    fg_point = np.array([[x_fg[len(x_fg)//2], y_fg[len(y_fg)//2], 1]])
    
    # èƒŒæ™¯ç‚¹ï¼ˆåœ¨è¾¹ç•Œå¤–ï¼‰
    y_bg, x_bg = np.where(mask_gt == 0)
    bg_point = np.array([[x_bg[0], y_bg[0], 0]])
    
    return np.concatenate([fg_point, bg_point], axis=0)
```

### 2. Fine-tuningæœ€ä½³å®è·µ

```python
# é’ˆå¯¹ç‰¹å®šæ¨¡æ€/ä»»åŠ¡fine-tune

# 1. æ•°æ®å‡†å¤‡
train_dataset = MedicalDataset(
    images=ct_images,
    masks=ct_masks,
    transform=augmentation
)

# 2. å­¦ä¹ ç‡è°ƒåº¦
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=20,
    eta_min=1e-7
)

# 3. æ—©åœç­–ç•¥
best_dice = 0
patience = 5
counter = 0

for epoch in range(50):
    train_dice = train_epoch(model, train_loader)
    val_dice = validate(model, val_loader)
    
    if val_dice > best_dice:
        best_dice = val_dice
        save_checkpoint(model)
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping")
            break
```

### 3. åå¤„ç†ä¼˜åŒ–

```python
def refine_prediction(pred_mask):
    """åå¤„ç†æå‡åˆ†å‰²è´¨é‡"""
    import cv2
    from scipy import ndimage
    
    # 1. ç§»é™¤å°è¿é€šåŸŸ
    labeled, num = ndimage.label(pred_mask)
    sizes = ndimage.sum(pred_mask, labeled, range(num + 1))
    mask_size = sizes < 100  # ç§»é™¤å°äº100åƒç´ çš„åŒºåŸŸ
    remove_pixel = mask_size[labeled]
    pred_mask[remove_pixel] = 0
    
    # 2. å½¢æ€å­¦é—­æ“ä½œï¼ˆå¡«å……å°å­”ï¼‰
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    pred_mask = cv2.morphologyEx(pred_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)
    
    # 3. è¾¹ç•Œå¹³æ»‘
    contours, _ = cv2.findContours(pred_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    smooth_mask = np.zeros_like(pred_mask)
    for contour in contours:
        epsilon = 0.001 * cv2.arcLength(contour, True)
        approx = cv2.approxPolyDP(contour, epsilon, True)
        cv2.drawContours(smooth_mask, [approx], -1, 1, -1)
    
    return smooth_mask
```

---

## ğŸ“– æ€»ç»“

### SAMçš„æ ¸å¿ƒè´¡çŒ®

1. **Promptable SegmentationèŒƒå¼**
   - é€šè¿‡promptå®ç°çµæ´»äº¤äº’
   - ä¸€ä¸ªæ¨¡å‹å¤„ç†å¤šç§ä»»åŠ¡

2. **SA-1Bè¶…å¤§è§„æ¨¡æ•°æ®é›†**
   - 11äº¿maskï¼Œå‰æ‰€æœªæœ‰çš„è§„æ¨¡
   - æ•°æ®é£è½®ï¼šæ¨¡å‹æ ‡æ³¨ â†’ æ”¹è¿›æ¨¡å‹

3. **Zero-shotæ³›åŒ–èƒ½åŠ›**
   - æ— éœ€è®­ç»ƒå³å¯åˆ†å‰²æ–°ç±»åˆ«
   - å¼€å¯åŸºç¡€æ¨¡å‹åœ¨è§†è§‰é¢†åŸŸçš„åº”ç”¨

### MedSAMçš„è´¡çŒ®

1. **åŒ»å­¦é¢†åŸŸé€‚é…**
   - 157ä¸‡åŒ»å­¦å›¾åƒfine-tune
   - è·¨æ¨¡æ€é€šç”¨æ€§

2. **Few-shoté«˜æ•ˆå­¦ä¹ **
   - 5-10ä¸ªæ ·æœ¬å³å¯é€‚é…æ–°ä»»åŠ¡
   - æ˜¾è‘—é™ä½æ ‡æ³¨æˆæœ¬

3. **ä¸´åºŠå®ç”¨æ€§**
   - äº¤äº’å¼åˆ†å‰²è¾…åŠ©è¯Šæ–­
   - åŠ é€Ÿæ•°æ®æ ‡æ³¨æµç¨‹

### æœªæ¥å±•æœ›

**æŠ€æœ¯æ–¹å‘**ï¼š
- **MedSAM 2.0**ï¼šæ”¯æŒ3DåŒ»å­¦å›¾åƒ
- **æ–‡æœ¬Prompt**ï¼šç»“åˆCLIPå®ç°"åˆ†å‰²è‚è„è‚¿ç˜¤"ç­‰è‡ªç„¶è¯­è¨€æŒ‡ä»¤
- **è½»é‡åŒ–**ï¼šMobileMedSAMç”¨äºç§»åŠ¨ç«¯

**åº”ç”¨å‰æ™¯**ï¼š
- æ”¾å°„ç§‘ï¼šè¾…åŠ©é˜…ç‰‡å’Œæµ‹é‡
- ç—…ç†ç§‘ï¼šå¿«é€Ÿæ ‡æ³¨å’Œè¯Šæ–­
- å¤–ç§‘ï¼šæœ¯å‰è§„åˆ’å’Œå¯¼èˆª
- ç ”ç©¶ï¼šé«˜æ•ˆæ•°æ®é›†æ„å»º

**ä¸‹ä¸€ç¯‡é¢„å‘Š**ï¼š[nnU-Netï¼šè‡ªé€‚åº”åŒ»å­¦åˆ†å‰²æ¡†æ¶](/2025/03/15/nnunet-self-configuring-framework/) - æ¢ç´¢å¦‚ä½•é€šè¿‡è‡ªåŠ¨åŒ–é…ç½®è®©UNeté€‚é…ä»»ä½•åŒ»å­¦åˆ†å‰²ä»»åŠ¡ã€‚

---

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡
1. [SAM] Kirillov, A., et al. (2023). Segment Anything. *ICCV*.
2. [MedSAM] Ma, J., et al. (2024). Segment Anything in Medical Images. *Nature Communications*.
3. [SAM-Med2D] Cheng, J., et al. (2023). SAM-Med2D. *arXiv*.

### ä»£ç å®ç°
- [SAMå®˜æ–¹](https://github.com/facebookresearch/segment-anything) - Meta AIåŸå§‹ä»£ç 
- [MedSAMå®˜æ–¹](https://github.com/bowang-lab/MedSAM) - åŒ»å­¦å›¾åƒç‰ˆæœ¬
- [SAM-Med2D](https://github.com/uni-medical/SAM-Med2D) - å¦ä¸€åŒ»å­¦SAMç‰ˆæœ¬

### æ•°æ®é›†
- [SA-1B](https://ai.facebook.com/datasets/segment-anything/) - SAMè®­ç»ƒæ•°æ®é›†
- [MedSAMæ•°æ®](https://github.com/bowang-lab/MedSAM#dataset) - åŒ»å­¦å›¾åƒæ•°æ®é›†é“¾æ¥

---

## ğŸ”— ç³»åˆ—æ–‡ç« å¯¼èˆª

1. [FCNä¸UNetï¼šåŒ»å­¦åˆ†å‰²çš„å¥ åŸºä¹‹ä½œ](/2025/02/01/fcn-unet-foundation/)
2. [V-Netï¼š3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„çªç ´](/2025/02/05/vnet-3d-segmentation/)
3. [Attention UNetï¼šæ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥](/2025/02/10/attention-unet/)
4. [UNet++/UNet 3+ï¼šå¯†é›†è¿æ¥çš„åŠ›é‡](/2025/02/15/unet-plus-series/)
5. [TransUNetï¼šCNNä¸Transformerçš„èåˆ](/2025/02/20/transunet-hybrid-architecture/)
6. [Swin-UNetï¼šå±‚çº§åŒ–Transformer](/2025/02/25/swin-unet-hierarchical-transformer/)
7. ğŸ“ **SAMä¸MedSAMï¼šåŸºç¡€æ¨¡å‹çš„åŒ»å­¦åº”ç”¨**ï¼ˆæœ¬æ–‡ï¼‰
8. [nnU-Netï¼šè‡ªé€‚åº”åŒ»å­¦åˆ†å‰²æ¡†æ¶](/2025/03/15/nnunet-self-configuring-framework/)

---

*æœ¬æ–‡æ·±å…¥æ¢è®¨äº†SAMå¦‚ä½•é€šè¿‡Promptable Segmentationé©æ–°åˆ†å‰²èŒƒå¼ï¼Œä»¥åŠMedSAMå¦‚ä½•å°†å…¶æˆåŠŸè¿ç§»åˆ°åŒ»å­¦å½±åƒé¢†åŸŸï¼Œå¼€å¯few-shotå­¦ä¹ å’Œäº¤äº’å¼åˆ†å‰²çš„æ–°æ—¶ä»£ã€‚*

